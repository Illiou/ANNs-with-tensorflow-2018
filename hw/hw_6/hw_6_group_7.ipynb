{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 6 Group 7\n",
    "- First download the dataset (\"bible.txt\") from Stud.IP and put it on the same level as this notebook\n",
    "\n",
    "## General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General tensorflow settings\n",
    "config = tf.ConfigProto()\n",
    "# Use GPU in incremental mode (is ignored on CPU version)\n",
    "config.gpu_options.allow_growth=True\n",
    "# Add config=config in every tf.Session() -> tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def tokenize_text(text):\n",
    "    text_lower = text.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text_tokenized = tokenizer.tokenize(text_lower)\n",
    "    return text_tokenized\n",
    "\n",
    "def create_dicts_from_tokenized_text(tokenized_text, vocabulary_size):\n",
    "    words_and_count = Counter(tokenized_text).most_common(vocabulary_size - 1)\n",
    "    # print(words_and_count)\n",
    "    word2id = {word: word_id for word_id, (word, _) in enumerate(words_and_count, 1)}\n",
    "    word2id[\"_UNKNOWN_\"] = 0\n",
    "    id2word = dict(zip(word2id.values(), word2id.keys()))\n",
    "    return word2id, id2word\n",
    "\n",
    "def find_and_print_nearest_neighbors(target_words, number_of_nearest_neighbors):\n",
    "    embedding_values = sess.run(embeddings)\n",
    "    normed_embeddings = embedding_values / np.sqrt(np.sum(embedding_values**2, axis=1, keepdims=True))\n",
    "    for word in target_words:\n",
    "        word_id = word2id[word]\n",
    "        word_embedding = normed_embeddings[word_id, :]\n",
    "        cosine_similarities = np.matmul(normed_embeddings, word_embedding )\n",
    "        n_nearest_neighbors = np.argsort(-cosine_similarities)[:number_of_nearest_neighbors]\n",
    "        print(\"Nearest to \" + word + \": \" + \", \".join([id2word[nearest] for nearest in n_nearest_neighbors]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_size = 64\n",
    "\n",
    "with open('bible.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text_tokenized = tokenize_text(text)\n",
    "\n",
    "word2id, id2word = create_dicts_from_tokenized_text(text_tokenized, vocab_size)\n",
    "\n",
    "text_ids = [word2id.get(word, 0) for word in text_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "854033\n",
      "[1, 253, 447, 3, 161]\n",
      "[447, 447, 447, 447, 3, 3, 3, 3, 161, 161]\n",
      "[1, 253, 3, 161, 253, 447, 161, 193, 447, 3]\n",
      "3416116 3416116\n"
     ]
    }
   ],
   "source": [
    "print(len(text_ids))\n",
    "print(text_ids[:5])\n",
    "\n",
    "# Create the training and context words \n",
    "# Ignore the first two and the last two words, because they don't have a valid context\n",
    "context_words = []\n",
    "training_words = []\n",
    "for i in range(2, len(text_ids)-2):\n",
    "    for j in [-2,-1,1,2]:\n",
    "        training_words.append(text_ids[i])\n",
    "        context_words.append(text_ids[i+j])\n",
    "\n",
    "print(training_words[:10])\n",
    "print(context_words[:10])\n",
    "print(len(training_words), len(context_words))\n",
    "\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices((training_words, context_words))\n",
    "training_dataset = training_dataset.shuffle(buffer_size=len(training_words), reshuffle_each_iteration=True)\n",
    "\n",
    "training_batch_size = 128\n",
    "training_dataset = training_dataset.batch(training_batch_size)\n",
    "\n",
    "# First we create the iterator\n",
    "iterator = tf.data.Iterator.from_structure(training_dataset.output_types,\n",
    "                                           training_dataset.output_shapes)\n",
    "\n",
    "# We name the get_next method of the iterator to use it as a shortcut\n",
    "next_batch = iterator.get_next()\n",
    "\n",
    "# We prepare the initializer operations for both the training and the validation dataset\n",
    "training_init_op = iterator.make_initializer(training_dataset)\n",
    "\n",
    "input_data = next_batch[0]\n",
    "input_goal = tf.expand_dims(next_batch[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"IteratorGetNext:0\", shape=(?,), dtype=int32) Tensor(\"ExpandDims:0\", shape=(?, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(input_data, input_goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'embedding/weight_matrix:0' shape=(10000, 64) dtype=float32_ref>\n",
      "<tf.Variable 'embedding/bias:0' shape=(10000,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"embedding\", reuse=tf.AUTO_REUSE) as scope:\n",
    "    uni_initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    embeddings = tf.get_variable(\"embeddings\", [vocab_size, embedding_size], initializer=uni_initializer)\n",
    "    \n",
    "    norm_initializer = tf.truncated_normal_initializer(stddev=1.0/np.sqrt(embedding_size))\n",
    "    weight_matrix = tf.get_variable(\"weight_matrix\", [vocab_size, embedding_size], initializer=norm_initializer)\n",
    "    \n",
    "    bias_initializer = tf.zeros_initializer()\n",
    "    biases = tf.get_variable(\"bias\", [vocab_size], initializer=bias_initializer)\n",
    "\n",
    "    print(weight_matrix)\n",
    "    print(biases)\n",
    "    input_emb = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    batch_losses = tf.nn.nce_loss(weight_matrix, biases, input_goal, input_emb, 64, vocab_size)\n",
    "    \n",
    "    loss = tf.reduce_mean(batch_losses)\n",
    "    \n",
    "    # Specify the variables for the summaries\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    # Now we will merge our summary scalars\n",
    "    merged_summaries = tf.summary.merge_all()\n",
    "\n",
    "    # We also have too specify summary file writers \n",
    "    train_writer = tf.summary.FileWriter('./summaries/train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before epoch: 0\n",
      "Nearest to israel: israel, akan, taketh, ithrite, may, hewer, salah, avenge\n",
      "Nearest to sin: sin, slaughter, mahershalalhashbaz, stamp, islands, embalm, penny, milcom\n",
      "Nearest to god: god, counsellor, earthquake, india, wound, agreeth, smoking, nophah\n",
      "Nearest to 5: 5, mehujael, nourished, vesture, ellasar, wedding, comforters, tilgathpilneser\n",
      "Nearest to make: make, drove, sorely, dregs, amongst, valiant, weariness, danced\n",
      "Nearest to jesus: jesus, custody, languages, husbandmen, pleased, slippery, haft, mealtime\n",
      "Nearest to year: year, bedstead, andrew, jemuel, mithredath, mounts, phalti, diminish\n",
      "\n",
      "Before epoch: 1\n",
      "Nearest to israel: israel, pilled, ithrite, hewer, taketh, death, seeth, tables\n",
      "Nearest to sin: sin, slaughter, 88, islands, mahershalalhashbaz, throne, sun, chaff\n",
      "Nearest to god: god, filthiness, thank, lord, o, mocked, earthquake, psalms\n",
      "Nearest to 5: 5, 8, 4, 15, 19, 14, 10, 6\n",
      "Nearest to make: make, executeth, seekest, corinth, lilies, fetters, lend, sanctify\n",
      "Nearest to jesus: jesus, pleased, contempt, escape, esaias, surname, slippery, passing\n",
      "Nearest to year: year, bedstead, andrew, mithredath, confident, estate, amalekite, subdued\n",
      "\n",
      "Before epoch: 2\n",
      "Nearest to israel: israel, judah, solomon, pilled, forthwith, tables, seeth, entry\n",
      "Nearest to sin: sin, 88, slaughter, meat, seeing, determined, soles, islands\n",
      "Nearest to god: god, thank, filthiness, lord, hosts, mocked, riot, forgive\n",
      "Nearest to 5: 5, 8, 4, 3, 13, 6, 14, 10\n",
      "Nearest to make: make, give, executeth, answer, proceeded, sit, counsellor, forget\n",
      "Nearest to jesus: jesus, pleased, surname, esaias, contempt, escape, passing, wit\n",
      "Nearest to year: year, bedstead, amalekite, book, andrew, lighteneth, confident, sidon\n",
      "\n",
      "Before epoch: 3\n",
      "Nearest to israel: israel, judah, solomon, entry, hezekiah, ammon, esau, forthwith\n",
      "Nearest to sin: sin, meat, 88, seeing, soles, slaughter, determined, imagine\n",
      "Nearest to god: god, thank, filthiness, hosts, forgive, lord, riot, lords\n",
      "Nearest to 5: 5, 3, 8, 4, 10, 13, 14, 6\n",
      "Nearest to make: make, give, forget, proceeded, bury, bring, be, answer\n",
      "Nearest to jesus: jesus, pleased, surname, esaias, toe, rabbi, peter, maidservants\n",
      "Nearest to year: year, bedstead, amalekite, lighteneth, book, sidon, farther, andrew\n",
      "\n",
      "Before epoch: 4\n",
      "Nearest to israel: israel, judah, hezekiah, solomon, entry, ammon, forthwith, samaria\n",
      "Nearest to sin: sin, meat, 88, seeing, soles, trespass, imagine, determined\n",
      "Nearest to god: god, thank, hosts, filthiness, forgive, lord, riot, because\n",
      "Nearest to 5: 5, 3, 8, 10, 14, 13, 12, 4\n",
      "Nearest to make: make, give, forget, made, bring, proceeded, bury, answer\n",
      "Nearest to jesus: jesus, pleased, surname, toe, peter, rabbi, passing, esaias\n",
      "Nearest to year: year, bedstead, amalekite, lighteneth, book, sidon, day, farther\n",
      "\n",
      "Before epoch: 5\n",
      "Nearest to israel: israel, judah, hezekiah, entry, ammon, solomon, vaunt, kishon\n",
      "Nearest to sin: sin, meat, trespass, seeing, 88, soles, wench, imagine\n",
      "Nearest to god: god, forgive, hosts, thank, filthiness, riot, lords, lord\n",
      "Nearest to 5: 5, 3, 13, 8, 12, 10, 14, 4\n",
      "Nearest to make: make, give, forget, made, bring, proceeded, sanctify, bury\n",
      "Nearest to jesus: jesus, pleased, peter, surname, toe, passing, esaias, john\n",
      "Nearest to year: year, amalekite, lighteneth, bedstead, sidon, day, book, month\n",
      "\n",
      "Before epoch: 6\n",
      "Nearest to israel: israel, judah, hezekiah, ammon, vaunt, entry, edom, solomon\n",
      "Nearest to sin: sin, trespass, meat, seeing, wench, freewill, riddle, soles\n",
      "Nearest to god: god, hosts, forgive, liveth, riot, righteousness, lord, thank\n",
      "Nearest to 5: 5, 3, 12, 14, 4, 13, 10, 9\n",
      "Nearest to make: make, give, made, forget, proceeded, bury, sanctify, shew\n",
      "Nearest to jesus: jesus, peter, pleased, surname, john, paul, toe, passing\n",
      "Nearest to year: year, lighteneth, amalekite, sidon, bedstead, month, day, shinar\n",
      "\n",
      "Before epoch: 7\n",
      "Nearest to israel: israel, judah, ammon, hezekiah, edom, entry, vaunt, kishon\n",
      "Nearest to sin: sin, trespass, meat, seeing, freewill, riddle, wench, righteousness\n",
      "Nearest to god: god, forgive, hosts, riot, liveth, righteousness, zareathites, thank\n",
      "Nearest to 5: 5, 3, 14, 4, 12, 8, 13, 10\n",
      "Nearest to make: make, made, give, sanctify, forget, bury, proceeded, shew\n",
      "Nearest to jesus: jesus, peter, john, pleased, paul, surname, low, toe\n",
      "Nearest to year: year, amalekite, lighteneth, sidon, month, shinar, bedstead, day\n",
      "\n",
      "Before epoch: 8\n",
      "Nearest to israel: israel, judah, hezekiah, edom, ammon, vaunt, entry, kishon\n",
      "Nearest to sin: sin, trespass, seeing, meat, freewill, riddle, righteousness, bedchamber\n",
      "Nearest to god: god, hosts, forgive, riot, liveth, righteousness, salvation, thank\n",
      "Nearest to 5: 5, 3, 14, 12, 4, 8, 13, 10\n",
      "Nearest to make: make, made, give, sanctify, forget, bury, answer, shew\n",
      "Nearest to jesus: jesus, peter, john, low, paul, surname, pleased, passing\n",
      "Nearest to year: year, month, amalekite, sidon, lighteneth, shinar, day, farther\n",
      "\n",
      "Before epoch: 9\n",
      "Nearest to israel: israel, judah, edom, ammon, hezekiah, vaunt, entry, kishon\n",
      "Nearest to sin: sin, trespass, freewill, riddle, righteousness, seeing, meat, bedchamber\n",
      "Nearest to god: god, hosts, forgive, liveth, riot, salvation, righteousness, redeemer\n",
      "Nearest to 5: 5, 3, 14, 12, 4, 13, 6, 8\n",
      "Nearest to make: make, made, give, sanctify, keep, bury, forget, cover\n",
      "Nearest to jesus: jesus, peter, john, paul, low, passing, surname, pleased\n",
      "Nearest to year: year, month, sidon, shinar, lighteneth, amalekite, day, farther\n",
      "\n",
      "Before epoch: 10\n",
      "Nearest to israel: israel, judah, ammon, edom, hezekiah, vaunt, entry, kishon\n",
      "Nearest to sin: sin, trespass, righteousness, freewill, riddle, seeing, bedchamber, meat\n",
      "Nearest to god: god, forgive, hosts, liveth, riot, salvation, righteousness, redeemer\n",
      "Nearest to 5: 5, 14, 3, 4, 12, 13, 8, 10\n",
      "Nearest to make: make, made, give, sanctify, keep, bury, forget, cover\n",
      "Nearest to jesus: jesus, peter, john, paul, passing, low, disciples, surname\n",
      "Nearest to year: year, month, shinar, sidon, amalekite, lighteneth, elah, farther\n",
      "\n",
      "Before epoch: 11\n",
      "Nearest to israel: israel, judah, edom, ammon, hezekiah, vaunt, entry, assyria\n",
      "Nearest to sin: sin, trespass, righteousness, freewill, riddle, bedchamber, seeing, handful\n",
      "Nearest to god: god, forgive, hosts, liveth, riot, salvation, righteousness, redeemer\n",
      "Nearest to 5: 5, 14, 12, 3, 6, 8, 10, 13\n",
      "Nearest to make: make, made, give, sanctify, keep, cover, bury, forget\n",
      "Nearest to jesus: jesus, peter, john, paul, passing, low, disciples, tahanites\n",
      "Nearest to year: year, month, shinar, sidon, amalekite, day, lighteneth, elah\n",
      "\n",
      "Before epoch: 12\n",
      "Nearest to israel: israel, judah, edom, ammon, hezekiah, vaunt, entry, kishon\n",
      "Nearest to sin: sin, trespass, righteousness, bedchamber, freewill, riddle, handful, seeing\n",
      "Nearest to god: god, forgive, hosts, riot, liveth, salvation, redeemer, saviour\n",
      "Nearest to 5: 5, 14, 3, 4, 12, 8, 10, 13\n",
      "Nearest to make: make, made, give, sanctify, keep, cover, forget, bury\n",
      "Nearest to jesus: jesus, peter, john, paul, passing, disciples, low, prisoner\n",
      "Nearest to year: year, month, shinar, sidon, amalekite, topaz, lighteneth, elah\n",
      "\n",
      "Before epoch: 13\n",
      "Nearest to israel: israel, judah, edom, ammon, hezekiah, vaunt, david, assyria\n",
      "Nearest to sin: sin, trespass, righteousness, freewill, bedchamber, riddle, handful, meat\n",
      "Nearest to god: god, forgive, riot, hosts, redeemer, salvation, liveth, saviour\n",
      "Nearest to 5: 5, 14, 12, 3, 4, 8, 13, 6\n",
      "Nearest to make: make, made, give, sanctify, cover, keep, establish, succeedest\n",
      "Nearest to jesus: jesus, peter, john, paul, passing, prisoner, disciples, low\n",
      "Nearest to year: year, month, shinar, sidon, topaz, amalekite, day, elah\n",
      "\n",
      "Before epoch: 14\n",
      "Nearest to israel: israel, judah, edom, ammon, vaunt, hezekiah, midian, assyria\n",
      "Nearest to sin: sin, trespass, righteousness, bedchamber, freewill, handful, riddle, meat\n",
      "Nearest to god: god, forgive, redeemer, riot, liveth, salvation, hosts, saviour\n",
      "Nearest to 5: 5, 14, 4, 3, 12, 6, 8, 10\n",
      "Nearest to make: make, made, give, sanctify, keep, cover, establish, burn\n",
      "Nearest to jesus: jesus, peter, john, paul, prisoner, passing, disciples, lighteneth\n",
      "Nearest to year: year, month, shinar, topaz, sidon, amalekite, elah, farther\n"
     ]
    }
   ],
   "source": [
    "# First we specify the number of epochs\n",
    "epochs = 15\n",
    "\n",
    "# We safely create our tensorflow session and pass our config parameters (for correct GPU usage, if GPU available)\n",
    "with tf.Session(config=config) as sess:\n",
    "\n",
    "    # We initialize our variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # We define a step counter (for the summaries)\n",
    "    global_steps = 0\n",
    "\n",
    "    # We will run our training as often as specified in epochs\n",
    "    for ep in range(epochs):\n",
    "\n",
    "        # Training Part\n",
    "        # We have to load the training data into the iterator\n",
    "        sess.run(training_init_op) \n",
    "        # Validation Part\n",
    "        print(\"\\nBefore epoch: {}\".format(ep))\n",
    "        find_and_print_nearest_neighbors([\"israel\", \"sin\", \"god\", \"5\", \"make\", \"jesus\", \"year\"], 8)\n",
    "\n",
    "        # We have to loop over all our batches in every epoch\n",
    "        while True:\n",
    "            try:\n",
    "                # We train with one batch and read the summary and save it in the variable summary\n",
    "                _, summary = sess.run((training_step, merged_summaries))\n",
    "                \n",
    "                # We write the summary to the disk at the specified location\n",
    "                train_writer.add_summary(summary, global_steps)\n",
    "\n",
    "                # We update our step counter\n",
    "                global_steps += 1\n",
    "\n",
    "            # After we finished all batches, we catch the OutOfRangeError and break\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard loss screenshots\n",
    "\n",
    "- with outliers\n",
    "![](https://i.imgur.com/sH6N7kP.png)\n",
    "\n",
    "- without outliers\n",
    "![](https://i.imgur.com/oPXn11u.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
