{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import struct\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#loading the dataset \n",
    "data = read_idx('/Users/Ronja/Desktop/ANNh/hw2/MNISt/train-images.idx3-ubyte')\n",
    "data = (data/255) * 2 - 1\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# displaying the first 10 training examples\n",
    "fig, ax = plt.subplots(1,10,figsize=(10,10))\n",
    "for i in range(10): ax[i].imshow(data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def feed_forward_layer(x, hidden_n, activation_fn, normalize):\n",
    "    initializer = tf.random_normal_initializer(stddev=0.02)\n",
    "    weights = tf.get_variable(\"weights\", [x.shape[1], hidden_n], tf.float32, initializer)\n",
    "    biases = tf.get_variable(\"biases\", [hidden_n], tf.float32, tf.zeros_initializer())\n",
    "   \n",
    "    drive = tf.matmul(x, weights) + biases\n",
    "    if normalize:\n",
    "        drive = batch_norm(drive, [0])\n",
    "   \n",
    "    if activation_fn == 'linear':\n",
    "        return drive\n",
    "    else:\n",
    "        return activation_fn(drive)\n",
    "\n",
    "\n",
    "def conv_layer(x, kernels_n, kernel_size, stride_size, activation_fn, normalize):\n",
    "    initializer = tf.random_normal_initializer(stddev=0.02)\n",
    "    kernels = tf.get_variable(\"kernels\", [kernel_size, kernel_size, x.shape[-1], kernels_n], tf.float32, initializer)\n",
    "    biases = tf.get_variable(\"biases\", [kernels_n], tf.float32, tf.zeros_initializer())\n",
    "\n",
    "    drive = tf.nn.conv2d(x, kernels, strides = [1, stride_size, stride_size, 1], padding = \"SAME\") + biases\n",
    "    if normalize:\n",
    "        drive = batch_norm(drive, [0,1,2])\n",
    "    \n",
    "    return activation_fn(drive)\n",
    "\n",
    "\n",
    "def back_conv_layer(x, target_shape, kernel_size, stride_size, activation_fn, normalize):\n",
    "    initializer = tf.random_normal_initializer(stddev=0.02)\n",
    "    kernels = tf.get_variable(\"kernels\", [kernel_size, kernel_size, target_shape[-1], x.shape[-1]], tf.float32, initializer)\n",
    "    biases = tf.get_variable(\"biases\", [target_shape[-1]], tf.float32, tf.zeros_initializer())\n",
    "\n",
    "    drive = tf.nn.conv2d_transpose(x, kernels, target_shape, strides = [1, stride_size, stride_size, 1], padding = \"SAME\") + biases\n",
    "    if normalize:\n",
    "        drive = batch_norm(drive, [0,1,2])\n",
    "    \n",
    "    return activation_fn(drive)\n",
    "\n",
    "\n",
    "def flatten(x):\n",
    "    size = int(np.prod(x.shape[1:]))\n",
    "    return tf.reshape(x, [-1, size])\n",
    "\n",
    "\n",
    "def batch_norm(x, axes):\n",
    "    mean, var = tf.nn.moments(x, axes = axes)\n",
    "    offset_initializer = tf.constant_initializer(0.0)\n",
    "    offset = tf.get_variable(\"offset\", [x.shape[-1]], tf.float32, offset_initializer)\n",
    "    scale_initializer = tf.constant_initializer(1.0)\n",
    "    scale = tf.get_variable(\"scale\", [x.shape[-1]], tf.float32, scale_initializer)\n",
    "    return tf.nn.batch_normalization(x, mean, var, offset, scale, 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All convolutional and transposed-convolutional layers use ker- nel size 5 and stride size 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 5\n",
    "stride_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input to Generator\n",
    "In each training step the generator will get 32 random vectors of dimension 50. For that we need a placeholder. The random vectors will be generated later in the session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(32, 50), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "vectors = tf.placeholder(tf.float32, shape=(32,50))\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Layer 1\n",
    "The first layer is a fully connected feed forward layer with 1024 hidden nodes. Use ReLU activation function and batch norm. The 1024 nodes are then reshaped to 64 feature maps of size 4x4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"generator_layer1/Reshape:0\", shape=(32, 4, 4, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"generator_layer1\", reuse=tf.AUTO_REUSE):\n",
    "    x = vectors\n",
    "    hidden_n = 1024\n",
    "    activation_fn = tf.nn.relu\n",
    "    normalize = True\n",
    "    #applying feed_forward layer\n",
    "    feed_forward_output = feed_forward_layer(x, hidden_n, activation_fn, normalize)\n",
    "    #reshape to 64 feature maps of size 4x4\n",
    "    layer_1 = tf.reshape(tensor = feed_forward_output, shape = (32,4,4,64))\n",
    "    print(layer_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Layer 2\n",
    "This layer is a transposed convolutional layer. Layer 2 upsamples the previous 64 feature maps to 32 feature maps of size 7x7. Use ReLU activation function and batch norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"generator_layer2/Relu:0\", shape=(32, 7, 7, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"generator_layer2\", reuse=tf.AUTO_REUSE):\n",
    "    x = layer_1\n",
    "    target_shape = [32, 7, 7, 32]\n",
    "    activation_fn = tf.nn.relu\n",
    "    normalize = True\n",
    "    layer_2 = back_conv_layer(x, target_shape, kernel_size, stride_size, activation_fn, normalize)\n",
    "print(layer_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Layer 3\n",
    "This layer is a transposed convolutional layer. Layer 3 upsamples to 16 feature maps of size 14x14. Use ReLU activation function and batch norm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"generator_layer3/Relu:0\", shape=(32, 14, 14, 16), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"generator_layer3\", reuse=tf.AUTO_REUSE):\n",
    "    x = layer_2\n",
    "    target_shape = (32,14,14,16)\n",
    "    activation_fn = tf.nn.relu\n",
    "    normalize = True\n",
    "    layer_3 = back_conv_layer(x, target_shape, kernel_size, stride_size, activation_fn, normalize)\n",
    "print(layer_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Layer 4\n",
    "This layer is the last transposed convolutional layer and will generate the images. It will thus sample up to only one map of size 28x28. Do not use batch norm and use the TanH activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"generator_layer4/Tanh:0\", shape=(32, 28, 28, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"generator_layer4\", reuse=tf.AUTO_REUSE):\n",
    "    x = layer_3\n",
    "    target_shape = (32,28,28,1)\n",
    "    activation_fn = tf.nn.tanh\n",
    "    normalize = False\n",
    "    generator_layer_4 = back_conv_layer(x, target_shape, kernel_size, stride_size, activation_fn, normalize)\n",
    "    print(generator_layer_4)\n",
    "    #layer_4 = tf.reshape(layer_4, (32,28,28))\n",
    "    #print(layer_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input to Discriminator\n",
    "The discriminator will get the 32 previously generated images together with 32 original images as an input. Use as usual the tf.data API to build a dataset and an iterator that gives you batches of 32 of the original images. Use tf.concat to build one batch of 64 out of the original and the generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tensorflow dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "\n",
    "# define the batch size\n",
    "batchsize = 32\n",
    "\n",
    "# split the dataset into batches\n",
    "dataset = dataset.batch(batchsize)\n",
    "\n",
    "# shuffel the data (in each epoch)\n",
    "dataset = dataset.shuffle(buffer_size=4, reshuffle_each_iteration = True)\n",
    "\n",
    "# create an iterator\n",
    "iterator = tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)\n",
    "\n",
    "# name operation that gives next batch\n",
    "next_batch = iterator.get_next()\n",
    "\n",
    "# initializer operations for training and validation dataset\n",
    "initialize_iterator = iterator.make_initializer(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concat:0\", shape=(?, 28, 28, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#making next batch and output of generator compatible\n",
    "next_batch = tf.cast(next_batch, tf.float32)\n",
    "next_batch = tf.reshape(next_batch,[-1, 28, 28, 1])\n",
    "generator_layer_4 = tf.cast(generator_layer_4, tf.float32)\n",
    "\n",
    "#combining next batch and ouput of generator\n",
    "mixed_batch = tf.concat([generator_layer_4, next_batch], 0)\n",
    "print(mixed_batch)\n",
    "#labels_original = tf.cast(tf.ones(32), tf.int32)\n",
    "#labels_generated = tf.cast(tf.zeros(32), tf.int32)\n",
    "#labels = tf.concat([labels_original, labels_generated], 0)\n",
    "#print(mixed_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Layer 1, 2 & 3\n",
    "All three layers are normal convolutional layers with respectively 8, 16 and 32 feature maps. All use ”same”-padding, leaky ReLU activation function and batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concat:0\", shape=(?, 28, 28, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"discriminator1-3\", reuse=tf.AUTO_REUSE):\n",
    "    activation_fn = tf.nn.leaky_relu\n",
    "    normalize = True\n",
    "    \n",
    "    print((mixed_batch))\n",
    "    with tf.variable_scope(\"discriminator1\", reuse=tf.AUTO_REUSE):\n",
    "        x = tf.reshape(mixed_batch, (64,1,28,28))\n",
    "        kernels_n = 8\n",
    "        disc_layer1 = conv_layer(x, kernels_n, kernel_size, stride_size, activation_fn, normalize)\n",
    "    \n",
    "    with tf.variable_scope(\"discriminator2\", reuse=tf.AUTO_REUSE):\n",
    "        x = disc_layer1\n",
    "        kernels_n = 16\n",
    "        disc_layer2 = conv_layer(x, kernels_n, kernel_size, stride_size, activation_fn, normalize)\n",
    "    \n",
    "    with tf.variable_scope(\"discriminator3\", reuse=tf.AUTO_REUSE):\n",
    "        x = disc_layer2\n",
    "        kernels_n = 32\n",
    "        disc_layer3 = conv_layer(x, kernels_n, kernel_size, stride_size, activation_fn, normalize)\n",
    "    \n",
    "    #print(disc_layer1)\n",
    "    #print(disc_layer2)\n",
    "    #print(disc_layer3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Layer 4\n",
    "The last layer is a fully connected feed forward layer. For that we first need to flatten the feature maps of the last layer. The layer only has one output node as the discriminator only has to distinct between original images (label: 1) and generated images (label: 0). Do not use any activation function but do use batch norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"discriminator4/batchnorm/add_1:0\", shape=(64, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    " with tf.variable_scope(\"discriminator4\", reuse=tf.AUTO_REUSE):\n",
    "        x = tf.reshape(disc_layer3,(64,128))\n",
    "        hidden_n = 1\n",
    "        normalize = True\n",
    "        activation_fn = 'linear'\n",
    "        disc_layer4 = feed_forward_layer(x, hidden_n, activation_fn, normalize)\n",
    "print(disc_layer4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "The training is a bit trickier as usual as we have to train the generator and the discriminator separately. Both will be optimized with Adam (learning rate = 0.0004, beta1 = 0.5).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Generator\n",
    "You need 32 ones as labels (remember the generator wants to fool the discrim- inator). \n",
    "\n",
    "Calculate the loss with tf.nn.sigmoid cross entropy with logits. Pay attention that you feed the correct half of the logits (the logits that corre- spond to the generated images) to the loss function. \n",
    "\n",
    "Take the mean.\n",
    "\n",
    "We only want to train the generator to minimize this loss. Use the following code snippet to get all variables that are part of the generator.\n",
    "\n",
    "trainable variables = tf.trainable variables()\n",
    "\n",
    "generator variables = [var for var in trainable variables if \"generator\" in var.name]\n",
    "\n",
    "This presupposes you use sensible variable scopes for your layers.\n",
    "\n",
    "Now you can initialize an Adam optimizer and instruct it to minimize the loss. \n",
    "\n",
    "The minimize functions has a parameter var list that you can pass the above generated list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"training_generator/strided_slice:0\", shape=(32, 1), dtype=float32)\n",
      "Tensor(\"training_generator/logistic_loss:0\", shape=(32, 1), dtype=float32)\n",
      "Tensor(\"training_generator/Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"training_generator\", reuse=tf.AUTO_REUSE) as scope:    \n",
    "    logits = disc_layer4[0:32]\n",
    "    #logits = tf.reshape(logits, (32,))\n",
    "    print(logits)\n",
    "    labels = tf.ones((32,1))\n",
    "    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels = labels, logits = logits)\n",
    "    print(cross_entropy)\n",
    "    generator_loss = tf.reduce_mean(cross_entropy)\n",
    "    print(generator_loss)\n",
    "\n",
    "    trainable_variables = tf.trainable_variables()\n",
    "    generator_variables = [var for var in trainable_variables if \"generator\" in var.name]\n",
    "\n",
    "    #defining the learning rate\n",
    "    learning_rate = 4e-4\n",
    "\n",
    "    #defining the optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1 = 0.5)\n",
    "\n",
    "    #define training step\n",
    "    training_step_generator = optimizer.minimize(generator_loss, var_list = generator_variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Discriminator\n",
    "Now you need 64 labels (32 ones and 32 zeros). Again make sure that the order matches how you concatenated original and generated images as the input for the discriminator ([original, generated] or [generated, original]). The rest is exactly as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"training_discriminator/concat:0\", shape=(64, 1), dtype=float32)\n",
      "Tensor(\"discriminator4/batchnorm/add_1:0\", shape=(64, 1), dtype=float32)\n",
      "Tensor(\"training_discriminator/logistic_loss:0\", shape=(64, 1), dtype=float32)\n",
      "Tensor(\"training_discriminator/Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"training_discriminator\", reuse=tf.AUTO_REUSE) as scope: \n",
    "    labels_original = tf.cast(tf.ones((32,1)), tf.float32)\n",
    "    labels_generated = tf.cast(tf.zeros((32,1)), tf.float32)\n",
    "    labels = tf.concat([labels_original, labels_generated], 0)\n",
    "    print(labels)\n",
    "    logits = disc_layer4\n",
    "    print(logits)\n",
    "    \n",
    "    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels = labels, logits = logits)\n",
    "    print(cross_entropy)\n",
    "    discriminator_loss = tf.reduce_mean(cross_entropy)\n",
    "    print(discriminator_loss)\n",
    "\n",
    "    trainable_variables = tf.trainable_variables()\n",
    "    generator_variables = [var for var in trainable_variables if \"discriminator\" in var.name]\n",
    "\n",
    "    #defining the learning rate\n",
    "    learning_rate = 4e-4\n",
    "\n",
    "    #defining the optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1 = 0.5)\n",
    "\n",
    "    #define training step\n",
    "    training_step_discriminator = optimizer.minimize(discriminator_loss, var_list = generator_variables)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring\n",
    "Use TensorBoard to visualize the losses and the generated images.\n",
    "\n",
    "discriminator_loss_summary = tf.summary.scalar(\"discriminator-loss\", discriminator loss)\n",
    "\n",
    "loss_summaries = tf.summary.merge([generator loss summary, discriminator loss summary]) \n",
    "\n",
    "generated_images_summary = tf.summary.image(\"generated-images\", generated images, max_outputs = 32)\n",
    "\n",
    "Read out the loss summaries in each training step. The generated images as explained above only each 100th step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_loss_summary = tf.summary.scalar(\"discriminator-loss\", discriminator_loss)\n",
    "generator_loss_summary = tf.summary.scalar(\"generator-loss\", generator_loss)\n",
    "loss_summaries = tf.summary.merge([generator_loss_summary, discriminator_loss_summary])\n",
    "generated_images_summary = tf.summary.image(\"generated-images\", generator_layer_4, max_outputs = 32)\n",
    "#where to write the summaries\n",
    "train_writer = tf.summary.FileWriter('./summaries/train/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Generating Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model for 2 epochs.\n",
    "In the very beginning of your session once sample 32 random vectors of dimen- sion 50 from a uniform distribution (between -1 and 1). We will use these to visualize our progress in the generation.\n",
    "In each training step sample 32 new random vectors of dimension 50 from the same distribution. These will be fed to the generator for training. This is why they have to be different in each training step.\n",
    "Run the training step (both training steps) while feeding the previously gen- erated vector to the placeholder.\n",
    "After each 100th training step feed in the vector sampled at the beginning and read out the summaries for the generated images (do not train).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.distributions.Uniform(\"Uniform/\", batch_shape=(), event_shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#uniform distribution from which we sample in each training step\n",
    "distribution = tf.distributions.Uniform(-1.0,1.0)\n",
    "print(distribution)\n",
    "\n",
    "#defining the epochs\n",
    "epochs = 2\n",
    "\n",
    "#starting Tensorflow session\n",
    "with tf.Session() as sess:\n",
    "    #inizializing variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #making global step counter\n",
    "    global_step = 0\n",
    "    \n",
    "    #defining the epoch loop (runs through training and validation epoch number of times)\n",
    "    for _ in range(epochs):\n",
    "        #initializing iterator with training data for this epoch\n",
    "        sess.run(initialize_iterator)\n",
    "        \n",
    "        #going through all batches once\n",
    "        while True:\n",
    "            try:\n",
    "                #sampling from uniform distribution\n",
    "                #random_vectors = distribution.sample((32,50))\n",
    "                random_vectors = np.random.uniform(size=(32,50))\n",
    "                if((global_step % 100) == 0):\n",
    "                    summary_images = sess.run([generated_images_summary], feed_dict={vectors : random_vectors})\n",
    "                    train_writer.add_summary(summary_images[0], global_step)\n",
    "                else:\n",
    "                    #running training steps, feeding in random vectors and reading out summary\n",
    "                    summaries, _, _ = sess.run([loss_summaries, training_step_generator, training_step_discriminator], feed_dict={vectors : random_vectors})\n",
    "                    #saving the summary\n",
    "                    train_writer.add_summary(summaries, global_step)\n",
    "\n",
    "                global_step = global_step + 1\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
