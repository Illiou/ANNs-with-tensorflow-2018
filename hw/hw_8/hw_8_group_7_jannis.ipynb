{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW8 Group 7\n",
    "- First download the MNIST dataset from http://yann.lecun.com/exdb/mnist/ and put it in a folder called MNIST on the same level as this notebook (or change the base_path under `data preparation and visualization` to the folder where you saved the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# General tensorflow settings\n",
    "config = tf.ConfigProto()\n",
    "# Use GPU in incremental mode (is ignored on CPU version)\n",
    "config.gpu_options.allow_growth=True\n",
    "# Add config=config in every tf.Session() -> tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def feed_forward_layer(x, hidden_n, activation_fn, normalize):\n",
    "    initializer = tf.random_normal_initializer(stddev=0.02)\n",
    "    weights = tf.get_variable(\"weights\", [x.shape[1], hidden_n], tf.float32, initializer)\n",
    "    biases = tf.get_variable(\"biases\", [hidden_n], tf.float32, tf.zeros_initializer())\n",
    "   \n",
    "    drive = tf.matmul(x, weights) + biases\n",
    "    if normalize:\n",
    "        drive = batch_norm(drive, [0])\n",
    "   \n",
    "    if activation_fn == 'linear':\n",
    "        return drive\n",
    "    else:\n",
    "        return activation_fn(drive)\n",
    "\n",
    "\n",
    "def conv_layer(x, kernels_n, kernel_size, stride_size, activation_fn, normalize):\n",
    "    initializer = tf.random_normal_initializer(stddev=0.02)\n",
    "    kernels = tf.get_variable(\"kernels\", [kernel_size, kernel_size, x.shape[-1], kernels_n], tf.float32, initializer)\n",
    "    biases = tf.get_variable(\"biases\", [kernels_n], tf.float32, tf.zeros_initializer())\n",
    "\n",
    "    drive = tf.nn.conv2d(x, kernels, strides = [1, stride_size, stride_size, 1], padding = \"SAME\") + biases\n",
    "    if normalize:\n",
    "        drive = batch_norm(drive, [0,1,2])\n",
    "    \n",
    "    return activation_fn(drive)\n",
    "\n",
    "\n",
    "def back_conv_layer(x, target_shape, kernel_size, stride_size, activation_fn, normalize):\n",
    "    initializer = tf.random_normal_initializer(stddev=0.02)\n",
    "    kernels = tf.get_variable(\"kernels\", [kernel_size, kernel_size, target_shape[-1], x.shape[-1]], tf.float32, initializer)\n",
    "    biases = tf.get_variable(\"biases\", [target_shape[-1]], tf.float32, tf.zeros_initializer())\n",
    "\n",
    "    drive = tf.nn.conv2d_transpose(x, kernels, target_shape, strides = [1, stride_size, stride_size, 1], padding = \"SAME\") + biases\n",
    "    if normalize:\n",
    "        drive = batch_norm(drive, [0,1,2])\n",
    "    \n",
    "    return activation_fn(drive)\n",
    "\n",
    "\n",
    "def flatten(x):\n",
    "    size = int(np.prod(x.shape[1:]))\n",
    "    return tf.reshape(x, [-1, size])\n",
    "\n",
    "\n",
    "def batch_norm(x, axes):\n",
    "    mean, var = tf.nn.moments(x, axes = axes)\n",
    "    offset_initializer = tf.constant_initializer(0.0)\n",
    "    offset = tf.get_variable(\"offset\", [x.shape[-1]], tf.float32, offset_initializer)\n",
    "    scale_initializer = tf.constant_initializer(1.0)\n",
    "    scale = tf.get_variable(\"scale\", [x.shape[-1]], tf.float32, scale_initializer)\n",
    "    return tf.nn.batch_normalization(x, mean, var, offset, scale, 1e-6)\n",
    "\n",
    "# Helper function to read the data\n",
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Read all training  data\n",
    "base_path = '../hw_2/MNIST/'\n",
    "\n",
    "# Read training data \n",
    "training_data = read_idx('{}train-images.idx3-ubyte'.format(base_path))\n",
    "# Convert training data to range [-1,1]\n",
    "training_data = (training_data/255) * 2 -1\n",
    "\n",
    "# Print some information about the data\n",
    "print('Training data shape: {}'.format(training_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlIAAABRCAYAAAAZ1Ej0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAFHJJREFUeJzt3Wd0VFUXxvF/gsFKQEEJiigoCCqKVFFWsNEsiA1FUcDeKBYUAV9BUEQ0LoqAFMG2VJYF7IpGQURZ2FgKqIhdwNiiCIKA836Ytc9Mkkm7mTtzb3x+XwLJZObczMydc/fZZ++MSCSCiIiIiFReZroHICIiIhJWmkiJiIiIeKSJlIiIiIhHmkiJiIiIeKSJlIiIiIhHmkiJiIiIeKSJlIiIiIhHmkiJiIiIeKSJlIiIiIhHO6XywTIyMkJdRj0SiWSUd5vqfozV/fhAxxgGOsbqf3ygYwwDHaMiUiIiIiKeaSIlIiIi4pEmUiIiIiIeaSIlIiIi4pEmUiIiIiIeaSIVUm3atGHOnDnMmTOHHTt2sGPHDvf/1q1bp3t4IhJCEydOJBKJEIlE+OSTT/jkk0844IAD0j0sEV+88cYb5Ofnk5+fX6X70URKRERExKOU1pHyQ40aNahdu3aJ71977bUA7LbbbgAccsghAFxzzTXcc889APTp0weALVu2cNdddwEwevRo38dcFa1atQJg4cKFZGdnAxCJREt0XHjhhQD07NmTunXrpmeAKXLiiScC8NhjjwHQuXNnPv/883QOKSlGjhwJRF+HmZnR65zjjjsOgEWLFqVrWFKGWrVqscceewBwyimnALD33nsDkJeXx9atW9M2too68MADAejbty///vsvAC1atACgefPmfPvtt+kaWtI0a9YMgKysLHJzcwGYOnUqgDvm0ixYsACA8847D4B//vnHr2EmRVZWFscccwwAd955JwDHHntsOocUKPfddx8AxxxzDA8//HCV7y8UE6lGjRpRs2ZNAPfi6NSpEwB16tThrLPOKvc+fvjhBwAmTZrEGWecAcDGjRsBWLFiReA/pNq3bw/A008/DUDt2rXdBMqOw97cdevW5eijjwbgww8/LPIzP9nJqW7dujz77LO+Pla7du0AWL58ua+Pkyr9+/cH4OabbwaKntjteZZgsEmHPVcdO3bk8MMPT3jbBg0aMGjQoFQNzbOff/4ZgMWLF9OzZ880jyY5DjvsMCD23jrnnHMAyMzMZN999wVi77Py3mP2N5k+fToAQ4YM4c8//0z6mJOldu3avPnmmwBs2LABgJycHPfv/yoLmFx55ZUAbNu2jTfeeKPK96ulPRERERGPAh2RsmWs/Pz8hMt3FWFXHLZk8tdff7nloPXr1wPw+++/B3JZyJYlW7duzaOPPgpEr3CLW7NmDQB33303AE888QTvvPMOEDvucePG+T5eW4Jq2rSprxGpzMxMGjduDOASYTMyyu1SEGh2HLvsskuaR+Jdhw4d6Nu3LxBdaoVYVADgxhtvBGDdunVANKpsr+tly5alcqiV1rx5cyAaibjgggsA2HXXXYHoa+/7778HYtFhWxbr3bu3Wz767LPPUjrmyti0aRNAtVjCM3bOO/nkk5N2nxdddBEAs2fPdufYoMvJyXFf/+sRKVupycrKAmDJkiXMmzevyveriJSIiIiIR4GOSH333XcA/PrrrxWKSNlVbWFhIccffzwQyw165JFHfBqlfx544AEglhRfGit3YAmvixYtctGhI444wr8BFmNXa++++66vj9OgQQMuu+wyABfRCPLVfllOOukkAAYOHFjk+5999hmnnnoqAD/99FPKx1UZ5557LhDdOl+vXj0gFiF86623XOL1hAkTivxeRkaG+5kl8QaFnW/Gjx8PxI6xVq1aJW67Zs0aunXrBsSudO31WK9ePfc3CbI6deoAcOSRR6Z5JMmzcOFCoGREqqCggNmzZwO4DR3xOYmWh2tR1bALe7S+NLm5uYwYMQKIfUb+9ttvpd6+T58+Lpdx7dq1QCxKXlWBnkjZH2Xo0KHuQ+Wjjz4Coknj5uOPPwagS5cuQDRMbUsKgwcPTtl4k6VNmzZAbAdQ/BvBkuKff/55t/vQlkrsb/P7779zwgknlPhdv9lJyW+zZs1y/7ZlzTDq1KkTc+bMAShxoTBhwoTALrPstFP0tNG2bVsAZs6cCUSXohcvXgzAmDFjgGjofOeddwZwIfSuXbu6+3r//fdTM+hKsg0pl156aam3sZNxly5d3NLewQcf7P/gfGBpBI0aNSrxs3bt2rmJYVBfk4lMmzYNgPnz5xf5/rZt28pc4rLd0J9++imAS0yPv6+gvm4TsUT6MKcNJDJjxgyaNm0KwKGHHgpEzzelGT58uNvNbhfiK1asSMpYtLQnIiIi4lGgI1Jm/vz5rvKoJXNaCPqSSy5xkRlLmARYuXIlAJdffnkqh1ol8TWigCJ1ol5++WUgFsLs3LmzSyS3CI1tYV6xYoULVVtUq3Xr1q4UQrLZ8mH9+vV9uf/i4qM39rcKo379+hW52oXoUhiQlNomfrGE8vjIIESfC1sCi98abt+Lj0RBtCTJQw895OdQPbOt8sV98803ruSGlT+waBTEkszDxqLac+fOZdSoUUV+NmrUKAoLCwGYMmVKqofm2fbt24Giz09F2DLtnnvuWeJnVkYnDLXBimvbti3vvfdeuoeRNJs3b65QtM0+Vw844AD3uZjs6JwiUiIiIiIehSIiBZQofvbHH3+4f9t655NPPgmUX6U2iJo1a8bQoUOBWMTll19+AaJlGuzK/a+//gLgxRdf5MUXXyz3fm2L9g033OC2bSebJXPaY/nFIl5W+gDgxx9/9PUx/WDJxxdffLF7rdoV/9ixY9M2rooYM2YMw4cPB2K5F7a9f+TIkQmLFFpCaHGDBg1yUdSgsXOKRbRfe+01AL788ksKCgpK/b1URWX9MmbMmBIRqf8K2/Bgz32i89n//ve/lI7Jq+3bt7vPSPs8Oeigg9I5pKSx/MuWLVuyevVqIHGu0+677w7EIse77babi8g99dRTSR2TIlIiIiIiHoUmIlWcXTW1adPGbVO1reR29RgGtqPpnnvucZEdywOzcgLvv/9+laM9iXbjJIv1MTSWn5ZslgtXv359vvjiCyD2twoDay1ibX7iTZ48GcC1dQgauxIfPny4Kyny6quvArErvr///tvd3nIQunbt6l57toPUom7WvyyILGeostGZjh07+jCa1EpUEqC6sij9sGHD3I5LK2ERz3aGb9u2LXWDq4LCwkLefvttALfjPez2339/IBYx3L59u+upmyiynZeXB8TyHdetW+dbv8HQTqQssfyyyy5zSdS2DfvNN99021Pvv/9+ILj9yo466iigaK2T008/HQhvk9pk9L/Lzs6me/fuQCy5OT5Z2cK7tiQWBnY88bW9rM/TxIkT0zKm8lh9oauvvhqIvo9sAtWrV68St7cPI+seYKU8IBZOtwr8YWW982zpIF7Lli2L/H/p0qW+11VLtor2nws6u3CxZu52oR3PerYmOlZbph42bBgvvfQSUPRiQVLDaj9ZtwxLjZg8eXLCz0irDWU9Fs0dd9zh2xi1tCciIiLiUWgjUmbt2rVu5mnFDS+88EJ3FWJXjbad3PrrBYWFHzMyMtzsOhmRqHSG5/faa6+E37eSFbbEY1eIDRs2pGbNmkAs1J6Zmemu/qxivW053mmnnfjggw98Gr0/evXq5TqPmyVLltCvXz+g6OaJILHnJb46t0Vk9tlnHwAGDBgAQM+ePd3Vo1XZj0Qi7mrfqtDHlykJOitUaQX/brvtthKVsjMzM0u8z2xpcMCAAezYsSMFI5V4hx9+OM899xzgPa3BlsZmzJiRtHGlkxWjDAMr+tu3b99Sq9B37NiRW265BYh9ju61115uKc8+Z+yz3zqF+EERKRERERGPQh+RgtjaqbULycvL48QTTwTgzjvvBKLFuCC6ThqELfOWAGjFwiKRiLuCSobieQ6WLOkHixzZY02fPt1tkY9nuUF2pWAF8zZv3syqVasAePDBB4Fogr1F5qzXnBXD23XXXUPTW6+sBPOvvvoq8H30LLHckjn33ntvvv76ayBxXolFYiy/pEGDBq6Mx/PPP+/7eJMhKyvL5S7a89agQQMg+lq3Y7Tcp+7du7vIlbEr6jPPPNPlv9nfUlLDzjNltckqK3Jv5+gePXq4gshh1rNnz3QPocKsFMWsWbPcecaeoy+//BKIFhi1NlWWV7zffvu596qdsy6++GLfx1stJlLGeiP17t2b0047DYgt911xxRUANG3a1PXkSyfbhWdLJwUFBa4Olle2AzB+p5FVhLcQqB8sEdn6cFnTz+KsCbX1q7IaIOVV27VaPtbg9quvvqriiFPHdrQlOlEXX+oLIkvmt8TyF154wS3dWq852303d+5c1x/ziSeeAKITEPt30Nl7sXv37jzzzDNFfjZ69Ggg+n565513gNgSdn5+vlvSNPZaHTduXInXfdCrYieaXOTm5gLhqWz+6aefusbttlnFNkls2bIl4e9ccsklQMkG4mFlO4DDtGvPuiDY5/a2bdvcOej8888Hor1kAe699163Y98mVBkZGW7iZekIVtn+uOOOc+esZNPSnoiIiIhH1SoiZQoLC3nkkUeAWD8wC7Xn5ua6KxXraxYEW7du9ZwIb5Eo6703dOhQtwx27733ArGK6H4aP368L/dry7Qm0TJZ0NiSbfH+chCL4Hz++ecpHVNVWMK/RVpKY5ELu1L8999/Ax9BtLpBFnWyDgOAW9KxOl+FhYXub2Bb4lu2bOmW7ay0g0WoTj/9dFcK4vXXXwei7xO7qjZ+Lr1XVqLyB2eeeSYQS7q3pfggswh5Rbe9WyS/ukSkLBJqsrKyXIqL/W2CxlaObOxjx4510aniBg4c6BLIE9VvsyVdi8z5FY0CRaREREREPKtWESlLZj777LNp164dEItEmVWrVrF48eKUj608XhLNLephV9C2vrxgwQLOOuus5A0uYGxzQZBZdf34DvKWC1a8UFx1Yrl/8VGNIOdI1ahRwxV3tUJ+mzZtYtiwYUAs18vyNNq2bevyhCwhfc2aNVx11VVA7Oo3OzsbiOYLWkkPS/ZduHChe3zL34jvH5lu06dPB2LRgXiWrzhkyJCUjikVunXrlu4hJJVt5jEZGRlu9SKoLFpvOYr2/kikXr16JXIT+/Tp43Klja3O+EkRKRERERGPQh+ROuSQQ1y/HVvHz8nJKXE7K4q3fv36QPSQKr41t1evXgwePLjCv3/ddddx6623ArHu3paLYT36JH2s+F38a23q1KlAavLV0sV2RoXF5Zdf7iJRmzdvBqKRGIsoHn300UCs6GiPHj1c1O32228HojuMil85W/mHV155hVdeeQWIXi1DbPcRRN/HQROW0iLxLM/NchLz8/Mr1c5lwIABgW3T5JVFd+z5bN68uYsk2k7roKnIc2Cfd+ecc46L/Fr+07x58/wbXBlCN5GySZKdlK699lpXqycR67lnCYfJrNVUFZbIaV9zcnKYNGkSEKul9OuvvwLRk7lVarfq4A0bNnQJefbhZR/U1ZVNOps1a1ZuyYR0scRI20Ieb+nSpakeTsqFbXnEmjFDdJkPokvllnhsvQPj2c/GjRsHUOHK5Y8//niRr0FlifWWdH3QQQe5n9nFnt3GzwTeiurUqRMjRowAcKVtGjduXOaykJWusCr1eXl5JWqB2USstHIJYWEXBfvttx/XX399mkdTdTYJvOqqqygoKADghBNOSOeQtLQnIiIi4lUoIlL169d3224t0bN58+al3n7ZsmVMmDABiIU3g7CcV5YaNWq4mbYlitvyQNOmTUvcfunSpS6xNf6qujqz6F2iaE8QtGrVyvUPtNebbYu///77A1/FPBmaNGmS7iFUyoYNG1w5A0vEtagvxEoc2AaV+fPn88033wAVj0SF1cqVK4Giz2kQz6NTpkwpkXR80003sXHjxlJ/xyJXrVu3BoqWerCyONOmTQNiGwjCLhKJhLq6vpVuuPTSS4Ho8VgfxFQklJclmJ9IIiIiIiEQyIiUrV9bsa1WrVqVeaVruSdWfPLVV1+tVKJhOlifruXLlwO4cg0QywOrX7+++57lS9l27Mokplc3HTt2ZO7cuekeRgl16tQpsdHB+jpaQnN19/bbbwNl9zALktzcXNf+xqITBQUFLk/RCmeG+UreK7vat3ZbYWLlKCqqoKDA9YK0c2vYc6OKy87Odj3pwlBCpjgrG2KRqUcffZTbbrstnUNyAjOR6tChAxBN9Gzfvj0QTY4rje2wmTRpkmtMvGnTJp9HmTwWirSdhldccYWrTF7cxIkTXZjZGjb+F5XVfFSCwWq4WAPxJk2auGRlayIaJBs3bnRdEOyrRFn18tWrV9OiRYs0j6Z0/fv3d4nx/fr1K/f2a9eudZ8fNvGfMWNGifpD1UXv3r2BaPcM628aRraRx+q+WdpOEGhpT0RERMSjjPgkO98fLCOj1Ae76667gKJ9rsyqVat44YUXgFi1VlvGs4rDqRCJRMoNiZR1jGFQ3jGm4/isErgtt8ycOTNh1eWK8PM5zMnJ4cknnwSiW7IBvv76ayDxNnq/BOF1as/ZrFmzWLRoERDbTp+MPm1BOEa/BfG9mEzJfA5to4C97saOHeu6CsyfPx+ILQ0tWLCADRs2VH7AHgThdWrpIC1atHDV9ZPZay8Ix+i38o5RESkRERERjwITkQoDzbyr//GBjjEZrOLwvHnzXEkI659lVcKrktMYhGP0m96LOsYw0DEqIiUiIiLimSJSlaCZd/U/PtAxJlN2drZrz2Rb0o844gigarlSQTpGv+i9qGMMAx2jJlKVohdM9T8+0DGGgY6x+h8f6BjDQMeopT0RERERz1IakRIRERGpThSREhEREfFIEykRERERjzSREhEREfFIEykRERERjzSREhEREfFIEykRERERjzSREhEREfFIEykRERERjzSREhEREfFIEykRERERjzSREhEREfFIEykRERERjzSREhEREfFIEykRERERjzSREhEREfFIEykRERERjzSREhEREfFIEykRERERjzSREhEREfFIEykRERERjzSREhEREfFIEykRERERjzSREhEREfHo/5GvNX4FBPTnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x720 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the first 10 images from the training data set\n",
    "\n",
    "# Create a figure with 10 subplots\n",
    "fig, ax = plt.subplots(1, 10, figsize=(10,10))\n",
    "\n",
    "# Loop over the 10 subplots and print the corresponding image\n",
    "for i in range(10):\n",
    "    # Show the image with grayscale colormap\n",
    "    ax[i].imshow(training_data[i], cmap=\"gray\")\n",
    "    # Turn axis labeling off\n",
    "    ax[i].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = tf.data.Dataset.from_tensor_slices((training_data))\n",
    "training_dataset = training_dataset.shuffle(buffer_size=len(training_data), reshuffle_each_iteration=True)\n",
    "\n",
    "training_batch_size = 32\n",
    "training_dataset = training_dataset.batch(training_batch_size)\n",
    "\n",
    "# First we create the iterator\n",
    "iterator = tf.data.Iterator.from_structure(training_dataset.output_types,\n",
    "                                           training_dataset.output_shapes)\n",
    "\n",
    "# We name the get_next method of the iterator to use it as a shortcut\n",
    "next_batch = tf.reshape(tf.cast(iterator.get_next(), dtype=tf.float32),[-1, 28, 28, 1])\n",
    "\n",
    "# We prepare the initializer operations for the training  dataset\n",
    "training_init_op = iterator.make_initializer(training_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(32, 50), dtype=float32)\n",
      "Tensor(\"GENERATOR_1/Reshape:0\", shape=(32, 4, 4, 64), dtype=float32)\n",
      "Tensor(\"GENERATOR_2/Relu:0\", shape=(32, 7, 7, 32), dtype=float32)\n",
      "Tensor(\"GENERATOR_3/Relu:0\", shape=(32, 14, 14, 16), dtype=float32)\n",
      "Tensor(\"GENERATOR_4/Tanh:0\", shape=(32, 28, 28, 1), dtype=float32)\n",
      "Tensor(\"concat:0\", shape=(?, 28, 28, 1), dtype=float32)\n",
      "Tensor(\"DISCRIMINATOR_1/LeakyRelu:0\", shape=(?, 14, 14, 8), dtype=float32)\n",
      "Tensor(\"DISCRIMINATOR_2/LeakyRelu:0\", shape=(?, 7, 7, 16), dtype=float32)\n",
      "Tensor(\"DISCRIMINATOR_3/LeakyRelu:0\", shape=(?, 4, 4, 32), dtype=float32)\n",
      "Tensor(\"DISCRIMINATOR_4/batchnorm/add_1:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "kernel_size = 5\n",
    "stride_size = 2\n",
    "\n",
    "# Input generator 32 random vectors of dimension 50\n",
    "generator_input = tf.placeholder(shape=[32, 50], dtype=tf.float32)\n",
    "print(generator_input)\n",
    "\n",
    "with tf.variable_scope(\"GENERATOR_1\", reuse=tf.AUTO_REUSE) as scope:\n",
    "    gen_lay_1 = tf.reshape(feed_forward_layer(generator_input, 1024, tf.nn.relu, True), [-1,4,4,64])\n",
    "    print(gen_lay_1)\n",
    "\n",
    "with tf.variable_scope(\"GENERATOR_2\", reuse=tf.AUTO_REUSE) as scope:\n",
    "    gen_lay_2 = back_conv_layer(gen_lay_1, [32, 7, 7, 32], kernel_size, stride_size, tf.nn.relu, True)\n",
    "    print(gen_lay_2)\n",
    "\n",
    "with tf.variable_scope(\"GENERATOR_3\", reuse=tf.AUTO_REUSE) as scope:\n",
    "    gen_lay_3 = back_conv_layer(gen_lay_2, [32, 14, 14, 16], kernel_size, stride_size, tf.nn.relu, True)\n",
    "    print(gen_lay_3)\n",
    "\n",
    "with tf.variable_scope(\"GENERATOR_4\", reuse=tf.AUTO_REUSE) as scope:\n",
    "    gen_lay_4 = back_conv_layer(gen_lay_3, [32, 28, 28, 1], kernel_size, stride_size, tf.tanh, False)\n",
    "    print(gen_lay_4)\n",
    "    \n",
    "# Input discriminator 32 generated images + 32 original images\n",
    "discriminator_input = tf.concat([gen_lay_4, next_batch], 0)\n",
    "print(discriminator_input)\n",
    "\n",
    "with tf.variable_scope(\"DISCRIMINATOR_1\", reuse=tf.AUTO_REUSE) as scope:\n",
    "    disc_lay_1 = conv_layer(discriminator_input, 8, kernel_size, stride_size, tf.nn.leaky_relu, True)\n",
    "    print(disc_lay_1)\n",
    "\n",
    "with tf.variable_scope(\"DISCRIMINATOR_2\", reuse=tf.AUTO_REUSE) as scope:\n",
    "    disc_lay_2 = conv_layer(disc_lay_1, 16, kernel_size, stride_size, tf.nn.leaky_relu, True)\n",
    "    print(disc_lay_2)\n",
    "\n",
    "with tf.variable_scope(\"DISCRIMINATOR_3\", reuse=tf.AUTO_REUSE) as scope:\n",
    "    disc_lay_3 = conv_layer(disc_lay_2, 32, kernel_size, stride_size, tf.nn.leaky_relu, True)\n",
    "    print(disc_lay_3)\n",
    "    \n",
    "with tf.variable_scope(\"DISCRIMINATOR_4\", reuse=tf.AUTO_REUSE) as scope:\n",
    "    flat_disc_lay_3 = flatten(disc_lay_3)\n",
    "    disc_lay_4 = feed_forward_layer(flat_disc_lay_3, 1, \"linear\", True)\n",
    "    print(disc_lay_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"metrics_gen/logistic_loss:0\", shape=(32, 1), dtype=float32)\n",
      "Tensor(\"metrics_gen/Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"metrics_gen\", reuse=tf.AUTO_REUSE) as scope:    \n",
    "    \n",
    "    cross_entropy_gen = tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_lay_4[:32], labels=tf.ones(shape=(32,1)))\n",
    "    print(cross_entropy_gen)\n",
    "    loss_gen = tf.reduce_mean(cross_entropy_gen)\n",
    "    print(loss_gen)\n",
    "       \n",
    "with tf.variable_scope(\"optimizer_gen\", reuse=tf.AUTO_REUSE) as scope:\n",
    "    trainable_variables = tf.trainable_variables()\n",
    "    generator_variables = [var for var in trainable_variables if \"GENERATOR\" in var.name]\n",
    "    optimizer_gen = tf.train.AdamOptimizer(learning_rate=0.0004, beta1=0.5)\n",
    "    training_step_gen = optimizer_gen.minimize(loss_gen, var_list=generator_variables)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"metrics_disc/logistic_loss:0\", shape=(64, 1), dtype=float32)\n",
      "Tensor(\"metrics_disc/Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"metrics_disc\", reuse=tf.AUTO_REUSE) as scope:    \n",
    "    \n",
    "    cross_entropy_disc = tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_lay_4, labels=tf.concat([tf.zeros(shape=(32,1)), tf.ones(shape=(32,1))], 0))\n",
    "    print(cross_entropy_disc)\n",
    "    loss_disc = tf.reduce_mean(cross_entropy_disc)\n",
    "    print(loss_disc)\n",
    "       \n",
    "with tf.variable_scope(\"optimizer_disc\", reuse=tf.AUTO_REUSE) as scope:\n",
    "    trainable_variables = tf.trainable_variables()\n",
    "    discriminator_variables = [var for var in trainable_variables if \"DISCRIMINATOR\" in var.name]\n",
    "    optimizer_disc = tf.train.AdamOptimizer(learning_rate=0.0004, beta1=0.5)\n",
    "    training_step_disc = optimizer_disc.minimize(loss_disc, var_list=discriminator_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Generating Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitoring\n",
    "discriminator_loss_summary = tf.summary.scalar(\"discriminator-loss\",\n",
    "loss_disc)\n",
    "generator_loss_summary = tf.summary.scalar(\"generator-loss\",\n",
    "loss_gen)\n",
    "loss_summaries = tf.summary.merge([generator_loss_summary, discriminator_loss_summary])\n",
    "generated_images_summary = tf.summary.image(\"generated-images\", gen_lay_4, max_outputs=32)\n",
    "# We also have too specify summary file writers \n",
    "train_writer = tf.summary.FileWriter('./summaries/train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    step = 0\n",
    "    visu_rands = np.random.uniform(size=(32,50))\n",
    "    train_writer.add_graph(sess.graph)\n",
    "    \n",
    "    for epoch in range(2):\n",
    "          \n",
    "        # Load the dataset into the iterator.\n",
    "        sess.run(training_init_op)\n",
    "        \n",
    "\n",
    "        # Go through the dataset until it's empty.\n",
    "        while True:\n",
    "            try:\n",
    "                if (step % 100 == 0):\n",
    "                    # use visu_rands\n",
    "                    summary_images = sess.run([generated_images_summary], feed_dict={generator_input : visu_rands})\n",
    "                    \n",
    "                    # write summary\n",
    "                    train_writer.add_summary(summary_images[0], global_step = step)\n",
    "\n",
    "                else:\n",
    "                    # Get new rands \n",
    "                    feed_rands = np.random.uniform(size=(32,50))\n",
    "\n",
    "                    # Train\n",
    "                    summaries, _, _ = sess.run([loss_summaries, training_step_disc, training_step_gen], feed_dict={generator_input : feed_rands})\n",
    "                    \n",
    "                    # write summary\n",
    "                    train_writer.add_summary(summaries, global_step = step)\n",
    "                \n",
    "                step += 1\n",
    "               \n",
    "\n",
    "            # Stop if iterator is empty.\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
