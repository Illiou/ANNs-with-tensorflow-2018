{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 9 Group 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "# General tensorflow settings\n",
    "config = tf.ConfigProto()\n",
    "# Use GPU in incremental mode (is ignored on CPU version)\n",
    "config.gpu_options.allow_growth=True\n",
    "# Add config=config in every tf.Session() -> tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "discount_factor = 0.99\n",
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted_rewards = np.zeros_like(rewards, dtype=np.float32)\n",
    "    for i, reward in enumerate(reversed(rewards)):\n",
    "        discounted_rewards[-(i+1)] = discounted_rewards[-i] * discount_factor + reward\n",
    "        \n",
    "    normalized_rewards = ((discounted_rewards - np.mean(discounted_rewards)) / np.std(discounted_rewards))\n",
    "    return normalized_rewards\n",
    "\n",
    "def feed_forward_layer(x, hidden_n, activation_fn, normalize, stddev=0.02):\n",
    "    initializer = tf.random_normal_initializer(stddev=stddev)\n",
    "    weights = tf.get_variable(\"weights\", [x.shape[1], hidden_n], tf.float32, initializer)\n",
    "    biases = tf.get_variable(\"biases\", [hidden_n], tf.float32, tf.zeros_initializer())\n",
    "   \n",
    "    drive = tf.matmul(x, weights) + biases\n",
    "    if normalize:\n",
    "        drive = batch_norm(drive, [0])\n",
    "   \n",
    "    if activation_fn == 'linear':\n",
    "        return drive\n",
    "    else:\n",
    "        return activation_fn(drive)\n",
    "\n",
    "\n",
    "\n",
    "def flatten(x):\n",
    "    size = int(np.prod(x.shape[1:]))\n",
    "    return tf.reshape(x, [-1, size])\n",
    "\n",
    "\n",
    "def batch_norm(x, axes):\n",
    "    mean, var = tf.nn.moments(x, axes = axes)\n",
    "    offset_initializer = tf.constant_initializer(0.0)\n",
    "    offset = tf.get_variable(\"offset\", [x.shape[-1]], tf.float32, offset_initializer)\n",
    "    scale_initializer = tf.constant_initializer(1.0)\n",
    "    scale = tf.get_variable(\"scale\", [x.shape[-1]], tf.float32, scale_initializer)\n",
    "    return tf.nn.batch_normalization(x, mean, var, offset, scale, 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 50}\n",
      "Box(4,)\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "print(env.metadata)\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_state = env.reset()\n",
    "done = False\n",
    "episodes = 2000\n",
    "for _ in range(episodes):\n",
    "    while(not done):\n",
    "        env.render()\n",
    "        sampled_action = env.action_space.sample()\n",
    "        game_state, reward, done, _ = env.step(sampled_action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"HIDDEN_16/Tanh:0\", shape=(1, 8), dtype=float32)\n",
      "Tensor(\"OUT_16/Sigmoid:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"OUT_16/Reshape:0\", shape=(1, 2), dtype=float32)\n",
      "[<tf.Tensor 'optimizer_7/mul:0' shape=(4, 8) dtype=float32>, <tf.Tensor 'optimizer_7/mul_1:0' shape=(8,) dtype=float32>, <tf.Tensor 'optimizer_7/mul_2:0' shape=(8, 1) dtype=float32>, <tf.Tensor 'optimizer_7/mul_3:0' shape=(1,) dtype=float32>]\n",
      "[<tf.Tensor 'optimizer_7/Placeholder:0' shape=(4, 8) dtype=float32>, <tf.Tensor 'optimizer_7/Placeholder_1:0' shape=(8,) dtype=float32>, <tf.Tensor 'optimizer_7/Placeholder_2:0' shape=(8, 1) dtype=float32>, <tf.Tensor 'optimizer_7/Placeholder_3:0' shape=(1,) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "# placeholder for the current state of the environment (data)\n",
    "state = tf.placeholder(tf.float32, [1,4])\n",
    "\n",
    "with tf.variable_scope(\"HIDDEN\", reuse=tf.AUTO_REUSE) as scope:\n",
    "    hidden = feed_forward_layer(state, 8, tf.tanh, False, 0.002)\n",
    "    print(hidden)\n",
    "\n",
    "with tf.variable_scope(\"OUT\", reuse=tf.AUTO_REUSE) as scope:\n",
    "    out = feed_forward_layer(hidden, 1, tf.sigmoid, False, 0.002)\n",
    "    print(out)\n",
    "    log_out = tf.reshape(tf.log([out, 1-out]), shape=(1,2)) # apply log? explanation: log_probabilities(tf.concat(p, 1-p)) # does not make sense and throws erros\n",
    "    print(log_out)\n",
    "    action = tf.multinomial(log_out, num_samples=1)[0][0]\n",
    "    action_probability = log_out[:, tf.to_int32(action)]\n",
    "    \n",
    "with tf.variable_scope(\"optimizer\", reuse=tf.AUTO_REUSE) as scope:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.02)\n",
    "    gradients_and_variables = optimizer.compute_gradients(action_probability)\n",
    "    gradients = [gradient_and_variable[0] * -1 for gradient_and_variable in gradients_and_variables] \n",
    "    print(gradients)\n",
    "    gradient_placeholders = [tf.placeholder(tf.float32, gradient.shape) for gradient in gradients]\n",
    "    print(gradient_placeholders)\n",
    "    training_step = optimizer.apply_gradients(zip(gradient_placeholders, tf.trainable_variables()))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Score: 20.4\n",
      "Epoch: 1 Score: 25.7\n",
      "Epoch: 2 Score: 23.2\n",
      "Epoch: 3 Score: 19.6\n",
      "Epoch: 4 Score: 24.2\n",
      "Epoch: 5 Score: 28.0\n",
      "Epoch: 6 Score: 21.7\n",
      "Epoch: 7 Score: 20.9\n",
      "Epoch: 8 Score: 25.7\n",
      "Epoch: 9 Score: 23.3\n",
      "Epoch: 10 Score: 15.1\n",
      "Epoch: 11 Score: 20.5\n",
      "Epoch: 12 Score: 23.1\n",
      "Epoch: 13 Score: 21.4\n",
      "Epoch: 14 Score: 18.9\n",
      "Epoch: 15 Score: 27.6\n",
      "Epoch: 16 Score: 19.3\n",
      "Epoch: 17 Score: 23.7\n",
      "Epoch: 18 Score: 19.2\n",
      "Epoch: 19 Score: 29.4\n",
      "Epoch: 20 Score: 17.6\n",
      "Epoch: 21 Score: 34.3\n",
      "Epoch: 22 Score: 40.6\n",
      "Epoch: 23 Score: 33.0\n",
      "Epoch: 24 Score: 35.4\n",
      "Epoch: 25 Score: 31.0\n",
      "Epoch: 26 Score: 37.3\n",
      "Epoch: 27 Score: 40.0\n",
      "Epoch: 28 Score: 45.7\n",
      "Epoch: 29 Score: 36.8\n",
      "Epoch: 30 Score: 43.7\n",
      "Epoch: 31 Score: 48.8\n",
      "Epoch: 32 Score: 38.5\n",
      "Epoch: 33 Score: 50.8\n",
      "Epoch: 34 Score: 52.3\n",
      "Epoch: 35 Score: 45.7\n",
      "Epoch: 36 Score: 56.4\n",
      "Epoch: 37 Score: 66.9\n",
      "Epoch: 38 Score: 66.1\n",
      "Epoch: 39 Score: 87.7\n",
      "Epoch: 40 Score: 62.5\n",
      "Epoch: 41 Score: 99.5\n",
      "Epoch: 42 Score: 99.2\n",
      "Epoch: 43 Score: 92.6\n",
      "Epoch: 44 Score: 123.7\n",
      "Epoch: 45 Score: 135.5\n",
      "Epoch: 46 Score: 160.6\n",
      "Epoch: 47 Score: 158.2\n",
      "Epoch: 48 Score: 148.7\n",
      "Epoch: 49 Score: 160.7\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(50):\n",
    "        gradients_epoch = []\n",
    "        scores = []\n",
    "        for run in range(10):\n",
    "            game_state = env.reset()\n",
    "            done = False\n",
    "            gradient_list = []\n",
    "            reward_list = []\n",
    "            i = 0\n",
    "            while(not done):\n",
    "                if run == 0:\n",
    "                    env.render()\n",
    "                sampled_action, gradient = sess.run([action, gradients], feed_dict={state: np.reshape(game_state, (1,4))})\n",
    "                game_state, reward, done, _ = env.step(sampled_action)\n",
    "                #print(len(gradient))\n",
    "                #print(len(gradient[0]))\n",
    "                #print(len(gradient[0][0]))\n",
    "                #print(gradient[0])\n",
    "                #print(gradient[sampled_action])\n",
    "                gradient_list.append(gradient)\n",
    "                reward_list.append(reward)\n",
    "                i += 1\n",
    "            scores.append(i)\n",
    "            norm_rewards = discount_rewards(reward_list, discount_factor)\n",
    "            #print(np.array(gradient_list)[0][0].shape)\n",
    "            #print((np.array(norm_rewards)[:,None]*np.array(gradient_list)).shape)\n",
    "\n",
    "            gradients_epoch.append(np.sum(np.array(norm_rewards)[:, None]*np.array(gradient_list), axis=0))\n",
    "        \n",
    "        total_gradients = np.sum(np.array(gradients_epoch), axis=0)\n",
    "        #print(total_gradients)\n",
    "        #print(\"\\n\")\n",
    "        feed_dict = {placeholder: total_gradients[i]\n",
    "            for i, placeholder in enumerate(gradient_placeholders)}\n",
    "        sess.run(training_step, feed_dict = feed_dict)\n",
    "        print(\"Epoch: {} Score: {}\".format(epoch, np.mean(scores)))\n",
    "    env.close()\n",
    "                \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
