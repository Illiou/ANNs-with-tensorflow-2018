{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.data import unbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_texts(texts, cutoff_length):    \n",
    "    # cutoff all texts\n",
    "    cutoff = [text[:cutoff_length] for text in texts]\n",
    "    \n",
    "    # fill short texts with zeros\n",
    "    zeropad = [np.pad(text, (0,cutoff_length-len(text)), 'constant', constant_values=(0, 0)) for text in cutoff]\n",
    "    \n",
    "    return np.array(zeropad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "training, validation = tf.keras.datasets.imdb.load_data(\n",
    "                        path='imdb.npz',\n",
    "                        num_words=vocab_size,\n",
    "                        skip_top=0,\n",
    "                        maxlen=None,\n",
    "                        seed=113,\n",
    "                        start_char=1,\n",
    "                        oov_char=2,\n",
    "                        index_from=3\n",
    "                    )\n",
    "\n",
    "training_texts, training_labels = training\n",
    "validation_texts, validation_labels = validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_length = 300\n",
    "\n",
    "training_texts = prepare_texts(training_texts, cutoff_length)\n",
    "validation_texts = prepare_texts(validation_texts, cutoff_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had <UNK> working to watch this feeble excuse for a film it must have looked like a great idea on paper but on film it looks like no one in the film has a clue what is going on crap acting crap costumes i can't get across how <UNK> this is to watch save yourself an hour a bit of your life <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "word_to_id = tf.keras.datasets.imdb.get_word_index()\n",
    "word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "print(' '.join(id_to_word[id] for id in training_texts[2] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Generate the tf.data.Dataset from the above defined lists.\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices((training_texts, training_labels))\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((validation_texts, validation_labels))\n",
    "\n",
    "# Shuffle and batch the data.\n",
    "training_dataset = training_dataset.shuffle(buffer_size=1000000)\n",
    "training_batchsize = 250\n",
    "training_dataset = training_dataset.batch(training_batchsize)\n",
    "validation_batchsize = 2500\n",
    "validation_dataset = validation_dataset.batch(validation_batchsize)\n",
    "\n",
    "# Generate the iterator.\n",
    "iterator = tf.data.Iterator.from_structure(training_dataset.output_types,training_dataset.output_shapes)\n",
    "training_init_op = iterator.make_initializer(training_dataset)\n",
    "validation_init_op = iterator.make_initializer(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sequences and labels of the next batch.\n",
    "next_batch = iterator.get_next()\n",
    "sequences = next_batch[0]\n",
    "labels = next_batch[1]\n",
    "labels = tf.cast(labels, dtype=tf.float32)\n",
    "labels = tf.expand_dims(labels,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparams.\n",
    "lstm_size = 64\n",
    "embedding_size = 64\n",
    "subseq_length = 100 \n",
    "\n",
    "# Initialize placeholders to feed in the subsequences.\n",
    "subsequences = tf.placeholder(shape=[None, subseq_length], dtype=tf.int32)\n",
    "subsequences_labels = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "\n",
    "# Initialize placeholder to feed in cell and hidden state.\n",
    "init_hs = tf.placeholder(shape=[None, lstm_size], dtype=tf.float32)\n",
    "init_cs = tf.placeholder(shape=[None, lstm_size], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "\n",
    "with tf.variable_scope(\"LSTM\", reuse=tf.AUTO_REUSE) as scope:\n",
    "    \n",
    "    # Initialize list to save the hidden states and outputs of the sequence.\n",
    "    hs = []\n",
    "    \n",
    "    # Initialize Embeddings\n",
    "    embedding_initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    embedding = tf.get_variable(\"embedding\", [vocab_size, embedding_size], initializer = embedding_initializer)\n",
    "    \n",
    "    # Initialize all weights and biases.\n",
    "    initializer = tf.random_normal_initializer(stddev=0.125)\n",
    "    \n",
    "    # forget gate\n",
    "    Wxf = tf.get_variable(\"Wxf\", [embedding_size, lstm_size], initializer=initializer)\n",
    "    Whf = tf.get_variable(\"Whf\", [lstm_size, lstm_size], initializer=initializer)\n",
    "    bf = tf.get_variable(\"bf\", [lstm_size], initializer=initializer)\n",
    "    \n",
    "    # new candidate\n",
    "    Wxc = tf.get_variable(\"Wxc\", [embedding_size, lstm_size], initializer=initializer)\n",
    "    Whc = tf.get_variable(\"Whc\", [lstm_size, lstm_size], initializer=initializer)\n",
    "    bc = tf.get_variable(\"bc\", [lstm_size], initializer=initializer)\n",
    "    \n",
    "    # input gate\n",
    "    Wxi = tf.get_variable(\"Wxi\", [embedding_size, lstm_size], initializer=initializer)\n",
    "    Whi = tf.get_variable(\"Whi\", [lstm_size, lstm_size], initializer=initializer)\n",
    "    bi = tf.get_variable(\"bi\", [lstm_size], initializer=initializer)\n",
    "    \n",
    "    # output gate\n",
    "    Wxo = tf.get_variable(\"Wxo\", [embedding_size, lstm_size], initializer=initializer)\n",
    "    Who = tf.get_variable(\"Who\", [lstm_size, lstm_size], initializer=initializer)\n",
    "    bo = tf.get_variable(\"bo\", [lstm_size], initializer=initializer)\n",
    "        \n",
    "    # readout\n",
    "    Why = tf.get_variable(\"Why\", [lstm_size,1], initializer=initializer)\n",
    "    by = tf.get_variable(\"by\", [1], initializer=initializer)\n",
    "                         \n",
    "    \n",
    "    \n",
    "    # Get the embeddings.\n",
    "    subsequences_embed = tf.nn.embedding_lookup(embedding, subsequences)\n",
    "    \n",
    "    # Initialize the hidden and the cell state.\n",
    "    h_t = init_hs\n",
    "    c_t = init_cs\n",
    "    \n",
    "\n",
    "    for t in range(subseq_length):\n",
    "\n",
    "        # Read out the ith input \n",
    "        x_t = subsequences_embed[:,t]\n",
    "        \n",
    "        # forget gate\n",
    "        f_t = tf.sigmoid(tf.matmul(x_t, Wxf) + tf.matmul(h_t, Whf) + bf)\n",
    "        \n",
    "        # input gate\n",
    "        c_new_t = tf.tanh(tf.matmul(x_t, Wxc) + tf.matmul(h_t, Whc) + bc)\n",
    "        i_t = tf.sigmoid(tf.matmul(x_t, Wxi) + tf.matmul(h_t, Whi) + bi)\n",
    "        \n",
    "        # update cell state\n",
    "        c_t = f_t * c_t + i_t * c_new_t\n",
    "        \n",
    "        # output_gate\n",
    "        h_new_t = tf.tanh(c_t)\n",
    "        o_t = tf.sigmoid(tf.matmul(x_t, Wxo) + tf.matmul(h_t, Who) + bo)\n",
    "        \n",
    "        # update hidden state\n",
    "        h_t = o_t * h_new_t\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # save hidden state\n",
    "        hs.append(h_t)\n",
    "        \n",
    "\n",
    "    hs_mean = tf.reduce_mean(hs, axis=0) \n",
    "    \n",
    "    logits = tf.matmul(hs_mean, Why) + by\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"metrics\", reuse=tf.AUTO_REUSE) as scope:    \n",
    "    \n",
    "    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=subsequences_labels)\n",
    "    loss = tf.reduce_mean(cross_entropy, axis=0)\n",
    "    loss = loss[0]\n",
    "   \n",
    "\n",
    "    guesses = tf.nn.sigmoid(logits) > 0.5\n",
    "    accuracy = tf.equal(tf.cast(guesses, tf.float32), subsequences_labels)\n",
    "    accuracy = tf.reduce_mean(tf.cast(accuracy, tf.float32))\n",
    "\n",
    "    \n",
    "with tf.variable_scope(\"optimizer\", reuse=tf.AUTO_REUSE) as scope:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.03)\n",
    "    training_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.scalar('loss', loss)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(\"./summaries/train\", tf.get_default_graph())\n",
    "validation_writer = tf.summary.FileWriter(\"./summaries/validation\", tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    for epoch in range(3):\n",
    "    \n",
    "        \n",
    "        \n",
    "        # Load the dataset into the iterator.\n",
    "        sess.run(training_init_op)\n",
    "        \n",
    "\n",
    "        # Go through the dataset until it's empty.\n",
    "        while True:\n",
    "            try:\n",
    "                # Beginning of new sequences: set new hidden and cell state to zeros.\n",
    "                hs_feed_val = np.zeros([training_batchsize,lstm_size])\n",
    "                cs_feed_val = np.zeros([training_batchsize,lstm_size])\n",
    "                \n",
    "                # Get the new batch of sequences.\n",
    "                sequences_feed, sequences_labels_feed = sess.run([sequences, labels])\n",
    "                \n",
    "                # Feed in subsequences.\n",
    "                for i in range(cutoff_length//subseq_length):\n",
    "                    subsequences_feed = sequences_feed[:,i*subseq_length:(i+1)*subseq_length]\n",
    "                    hs_feed_val, cs_feed_val, summaries, _ = \\\n",
    "                                sess.run([h_t,c_t, merged_summaries, training_step], \n",
    "                                        feed_dict={init_hs: hs_feed_val,\n",
    "                                                   init_cs: cs_feed_val, \n",
    "                                                   subsequences:subsequences_feed,\n",
    "                                                   subsequences_labels: sequences_labels_feed})\n",
    "                \n",
    "                    train_writer.add_summary(summaries, global_step = step)\n",
    "                \n",
    "                    step += 1\n",
    "               \n",
    "\n",
    "            # Stop if iterator is empty.\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "                \n",
    "        \n",
    "        \n",
    "        sess.run(validation_init_op)\n",
    "        # Go through the dataset until it's empty.\n",
    "        while True:\n",
    "            try:\n",
    "                # Beginning of new sequences: set new hidden and cell state to zeros.\n",
    "                hs_feed_val = np.zeros([validation_batchsize,lstm_size])\n",
    "                cs_feed_val = np.zeros([validation_batchsize,lstm_size])\n",
    "\n",
    "                # Get the new batch of sequences.\n",
    "                sequences_feed, sequences_labels_feed = sess.run([sequences, labels])\n",
    "                \n",
    "                # Feed in subsequences.\n",
    "                for i in range(cutoff_length//subseq_length):\n",
    "                    subsequences_feed = sequences_feed[:,i*subseq_length:(i+1)*subseq_length]\n",
    "                    hs_feed_val, cs_feed_val, summaries, _ = \\\n",
    "                                sess.run([h_t,c_t, merged_summaries, training_step], \n",
    "                                        feed_dict={init_hs: hs_feed_val,\n",
    "                                                   init_cs: cs_feed_val, \n",
    "                                                   subsequences:subsequences_feed,\n",
    "                                                   subsequences_labels: sequences_labels_feed})\n",
    "                    \n",
    "                    validation_writer.add_summary(summaries, global_step = step)\n",
    "                \n",
    "                \n",
    "                    step += 1\n",
    "               \n",
    "\n",
    "            # Stop if iterator is empty.\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "                \n",
    "                \n",
    "       \n",
    "       \n",
    "      \n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
