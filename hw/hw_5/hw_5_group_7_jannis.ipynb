{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5 Group 7 \n",
    "\n",
    "- First download the dataset (\"holy_grail.txt\") from Stud.IP and put it on the same level as this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General tensorflow settings\n",
    "config = tf.ConfigProto()\n",
    "# Use GPU in incremental mode (is ignored on CPU version)\n",
    "config.gpu_options.allow_growth=True\n",
    "# Add config=config in every tf.Session() -> tf.Session(config=config)\n",
    "\n",
    "# We have to enable eager execution to loop over the input\n",
    "#tf.enable_eager_execution() # Not working with placeholders, alternative: tf.map_fn or tf.unstack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First read in the data\n",
    "text = open(\"holy_grail.txt\", 'r').read()\n",
    "#text = text[:1000]\n",
    "text_length = len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text contains 60061 letters and 78 unique characters\n"
     ]
    }
   ],
   "source": [
    "# We create our vocabulary\n",
    "vocab = set(text)\n",
    "vocab_len = len(vocab)\n",
    "# Create the index for each character and vice versa\n",
    "char_to_pos = {char:pos for pos, char in enumerate(vocab)}\n",
    "pos_to_char = {pos:char for pos, char in enumerate(vocab)}\n",
    "\n",
    "# Print some properties of our data\n",
    "print(\"The text contains {} letters and {} unique characters\".format(text_length, vocab_len))\n",
    "\n",
    "# Transform each character to the position of that character in the vocabulary\n",
    "text_number = [char_to_pos[char] for char in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the chunk with a sub sequence length of 25\n",
    "chunks = [text_number[i:i+25] for i in range(text_length-25)]\n",
    "# Construct the input sequences and the target sequences\n",
    "input_sequences = chunks[:-1]\n",
    "target_sequences = chunks[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset for tensorflow\n",
    "\n",
    "# First we reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Now we use the tf.data library to create a tensorflow dataset\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices((input_sequences, target_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the iterator \n",
    "\n",
    "# First we create the iterator\n",
    "iterator = tf.data.Iterator.from_structure(training_dataset.output_types,\n",
    "                                           training_dataset.output_shapes)\n",
    "\n",
    "# We prepare the initializer operations for the training\n",
    "training_init_op = iterator.make_initializer(training_dataset)\n",
    "\n",
    "# We name the get_next method of the iterator to use it as a shortcut\n",
    "next_batch = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"one_hot:0\", shape=(25, 78), dtype=float32) Tensor(\"one_hot_1:0\", shape=(25, 78), dtype=float32)\n",
      "Tensor(\"Placeholder:0\", shape=(1, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# First we will format the data in the correct way\n",
    "\n",
    "# We define our input data and the corresponding target data and transform it to one hot encoding\n",
    "input_data = tf.one_hot(next_batch[0], vocab_len)\n",
    "target_data = tf.one_hot(next_batch[1], vocab_len)\n",
    "\n",
    "print(input_data, target_data)\n",
    "\n",
    "hidden_len = 100\n",
    "# Create the placeholder for the hidden state\n",
    "rem_hidden_state = tf.placeholder(tf.float32, shape=(1,hidden_len))\n",
    "\n",
    "print(rem_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we build our RNN\n",
    "\n",
    "# We have to define our lists\n",
    "hidden_states = []\n",
    "logits = []\n",
    "\n",
    "# We define all our weights and biases\n",
    "w_xh = tf.Variable(tf.random_normal([vocab_len, hidden_len]))\n",
    "w_hh = tf.Variable(tf.random_normal([hidden_len, hidden_len]))\n",
    "b_h = tf.Variable(tf.random_normal([1, hidden_len]))\n",
    "w_hy = tf.Variable(tf.random_normal([hidden_len, vocab_len]))\n",
    "b_y = tf.Variable(tf.random_normal([1, vocab_len]))\n",
    "\n",
    "# We define the for_loop as a function \n",
    "#def forward_steps(char):\n",
    "for char in tf.unstack(input_data):\n",
    "    char = tf.reshape(char, [1, vocab_len])\n",
    "    new_hidden_state = tf.tanh(char @ w_xh + rem_hidden_state @ w_hh + b_h)\n",
    "    logit = new_hidden_state @ w_hy + b_y\n",
    "    hidden_states.append(new_hidden_state)\n",
    "    logits.append(logit)\n",
    "\n",
    "# Fancy for loop\n",
    "# tf.map_fn(forward_steps, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read out hidden states?\n",
    "rem_hidden = hidden_states[-1]\n",
    "\n",
    "# Read out logits of last iteration\n",
    "final_logits = logits[-1]\n",
    "output_softmax = tf.nn.softmax(final_logits)\n",
    "\n",
    "# Loss is mean of the cross entropies in each time step\n",
    "outputs = tf.concat(logits, axis=0)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=target_data, logits=outputs)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Optimizer\n",
    "# We create an optimizer\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "# And tell the optimizer that it should minimize the loss\n",
    "training_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "Ep: 0, Step: 60035, Loss: 2.577828884124756\n",
      "----\n",
      "\n",
      "  alannlcartlapal\n",
      "Ssk, akdktg\n",
      " t aetle t!\n",
      " atond\n",
      "Sg\n",
      " RTHARDIbrtyS atamst\n",
      " at!  afs\n",
      " amk,\n",
      " Skl, yomy!\n",
      " An\n",
      " bbae alaaboot I!\n",
      " Se!\n",
      "Stat!\n",
      "Staat!\n",
      " SttattstAy, t\n",
      " ld!\n",
      "St ted\n",
      "Sy AR: y\n",
      " totarsthabfyofot taot!\n",
      "----\n",
      "Ep: 1, Step: 120070, Loss: 2.284122943878174\n",
      "----\n",
      "\n",
      "!\n",
      " t ARDIy!\n",
      " billdt!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "   p!\n",
      " yold!\n",
      "  y MI  Sntgang g St St AHUSnd rttanat Imlt    lory!\n",
      " tklk!\n",
      " R: ft Swe RDIyoandyt,  mat byoflstat tan RTHUR: tttat SlySmt  ofaftd!\n",
      " R: t\n",
      "  R: ndt\n",
      " my Att amatdt  \n",
      "----\n",
      "Ep: 2, Step: 180105, Loss: 2.4048447608947754\n",
      "----\n",
      "\n",
      "e y!\n",
      " abaondtot Sbrininanird  ARSbuld g  aER: Standtad  ARTHURTHURTHUAR:  AR: AR: Yof RDyys\n",
      "  R: Sk Snaby R: y\n",
      " ARTHUR: Smang e\n",
      " St St!\n",
      " :  tanat,  yly\n",
      " y, Stet\n",
      " Spanane t aaticet\n",
      "\n",
      " St\n",
      " RTHUR: f\n",
      " HURT\n",
      "----\n",
      "Ep: 3, Step: 240140, Loss: 2.527308702468872\n",
      "----\n",
      "\n",
      "at  AERTHUR: S: bk S:  S: at,  S: ARy otyS: att!\n",
      "ScyAR: AR: ARy!\n",
      " yotamyotet!\n",
      " HUR: ARTHURTHURTHURouraUR: AR: y\n",
      " ARTHURTbtfl, AR:  S: Otatth and\n",
      "\n",
      " S: TSt!\n",
      " I IR: S: arrat\n",
      " S: amit!\n",
      " St\n",
      "\n",
      " IRS: Sth!\n",
      " IR\n",
      "----\n",
      "Ep: 4, Step: 300175, Loss: 2.157513380050659\n",
      "----\n",
      "\n",
      "!\n",
      "\n",
      "\n",
      " bot  Stat ban Send!\n",
      " bl Samy!\n",
      "\n",
      "\n",
      " RTHUR: an SabERTHyotllcet Smd Stam, Sapdtas!\n",
      " Samy, Sttans RTHURBnckr\n",
      " THUR: Slat t!  t  SURTHURDamy  Sl.\n",
      "\n",
      "\n",
      "   bobencabind\n",
      " tand!\n",
      " SURTHUt!\n",
      " AR: yaam! ond!\n",
      " I SER\n",
      "----\n",
      "Ep: 5, Step: 360210, Loss: 2.3563835620880127\n",
      "----\n",
      "\n",
      "fly krtetsang at fty\n",
      " t!\n",
      " Satkingay nyotan, kminry.\n",
      "  RTHUR: af bnd!\n",
      " r t!\n",
      " at\n",
      " YURTHUR: nt\n",
      " bER: aty Stathabrinty S: y S: can Sotenad  blamttts r\n",
      " Skitlo YUSurySyobay\n",
      " STHUR: mleg me\n",
      "  gonggy arang!\n",
      "\n",
      "----\n",
      "Ep: 6, Step: 420245, Loss: 2.4146595001220703\n",
      "----\n",
      "\n",
      "RAR: Ss\n",
      " at\n",
      " St!\n",
      " tlit\n",
      "\n",
      " nse bt!\n",
      " ngd\n",
      " brt!\n",
      " Syt fbr at bs!\n",
      " y\n",
      " Sfyelimellleat fnt\n",
      " t\n",
      " ckob bapathatayad\n",
      "  Soble at\n",
      "\n",
      " bnd\n",
      " tat\n",
      " bs-tat, Scnk S: tatande thautase\n",
      " thgababfbkot St\n",
      " TS: At Wee yts afinda\n",
      "----\n",
      "Ep: 7, Step: 480280, Loss: 2.579906702041626\n",
      "----\n",
      "\n",
      "rdthstat\n",
      " th!\n",
      "\n",
      " ,\n",
      "   angeataat't\n",
      " benglitgoiffl\n",
      " Stpy\n",
      "\n",
      " tamake fr\n",
      "\n",
      " tlat!\n",
      "\n",
      "  betdatgeggeth\n",
      "  d\n",
      "  k\n",
      "\n",
      " at!\n",
      " gk bandge!\n",
      " tattyoffrptt t  t Sat!\n",
      " Syanth\n",
      " Satyot atst!\n",
      "  Sif mkay Steatat!\n",
      "\n",
      " ty\n",
      " Sk tligaway\n",
      "----\n",
      "Ep: 8, Step: 540315, Loss: 2.5347843170166016\n",
      "----\n",
      "\n",
      "yS: tyhafatralacoth ad!\n",
      "   Sint!\n",
      " af  THUR: Sincop   St\n",
      " te!\n",
      "\n",
      " t Sp nt\n",
      "\n",
      "   t! \n",
      " yiraand!\n",
      " y!\n",
      "  Sondamed\n",
      " So S: \n",
      " agh Scataf S, yyat\n",
      " t\n",
      " Salyotig atd SerafERTHURTHURTHUR: it Smes!\n",
      " ama, tat!\n",
      " Scfaag\n",
      " b\n",
      "----\n",
      "Ep: 9, Step: 600350, Loss: 2.843251943588257\n",
      "----\n",
      "\n",
      "Sonkestaf  y!\n",
      " by\n",
      " THUR: STHUR: S \n",
      " ygotgand t\n",
      " ARTHURTHURTHUR: \n",
      " atatc\n",
      " t adan  Sotleratrk botinmy\n",
      " AR: ARTHUR: SotbEMIthly  bt!\n",
      " bate ARTHat!\n",
      " Sy\n",
      " the Sindt!\n",
      " angamitat\n",
      " pyoy Sorirg Sat, SUR: THURTH\n"
     ]
    }
   ],
   "source": [
    "# Finally we can train our model\n",
    "\n",
    "# First we specify the number of epochs\n",
    "epochs = 10\n",
    "\n",
    "# We safely create our tensorflow session and pass our config parameters (for correct GPU usage, if GPU available)\n",
    "with tf.Session(config=config) as sess:\n",
    "    \n",
    "    # We initialize our variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # We define a step counter (for the summaries)\n",
    "    global_step = 0\n",
    "    \n",
    "    # We will run our training as often as specified in epochs\n",
    "    for ep in range(epochs):\n",
    "        \n",
    "        # Training Part\n",
    "        \n",
    "        # We initialize the hidden state with zeros in the beginning of each new epoch\n",
    "        hidden_state = np.zeros((1, hidden_len))\n",
    "        \n",
    "        # We have to load the training data into the iterator\n",
    "        sess.run(training_init_op)\n",
    "        \n",
    "        # We have to loop over all our batches in every epoch\n",
    "        while True:\n",
    "            try:\n",
    "                # print(hidden_state.shape, hidden_state.dtype)\n",
    "                # We train with one batch and read the summary and save it in the variable summary\n",
    "                _, hidden_state, loss_val = sess.run([training_step, rem_hidden, loss], feed_dict={rem_hidden_state: hidden_state})\n",
    "                # print(hidden_state)       \n",
    "                # We update our step counter\n",
    "                global_step += 1\n",
    "                \n",
    "            # After we finished all batches, we catch the OutOfRangeErrpr and break\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "                \n",
    "        # Print loss after each epoch\n",
    "        print(\"\\n----\\nEp: {}, Step: {}, Loss: {}\\n----\\n\".format(ep, global_step, loss_val))\n",
    "        \n",
    "        # Sampling\n",
    "        sample_length = 200\n",
    "        random_sub_sequences = [input_sequences[np.random.choice(len(input_sequences))]]\n",
    "        random_fake_targets = [target_sequences[np.random.choice(len(target_sequences))]]\n",
    "        #input_data = tf.one_hot(next_batch[0], vocab_len)\n",
    "        #target_data = tf.one_hot(next_batch[1], vocab_len)\n",
    "        sample_outputs = []\n",
    "        \n",
    "        for t in range(sample_length):\n",
    "            sample_dataset = tf.data.Dataset.from_tensor_slices((random_sub_sequences, random_fake_targets))\n",
    "            sess.run(iterator.make_initializer(sample_dataset))\n",
    "            sample_output_softmax, hidden_state = sess.run([output_softmax, rem_hidden], feed_dict={rem_hidden_state: hidden_state})\n",
    "            sample_output = np.random.choice(vocab_len, p=sample_output_softmax.ravel())\n",
    "            sample_outputs.append(sample_output)\n",
    "            # Delete the first character and append the newly sampled one\n",
    "            random_sub_sequences[0] = random_sub_sequences[0][1:] + [sample_output]\n",
    "        \n",
    "        # Print the sample\n",
    "        print(''.join(pos_to_char[pos] for pos in sample_outputs))           "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
