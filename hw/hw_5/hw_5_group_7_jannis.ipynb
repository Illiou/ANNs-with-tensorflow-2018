{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5 Group 7 \n",
    "\n",
    "- First download the dataset (\"holy_grail.txt\") from Stud.IP and put it on the same level as this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General tensorflow settings\n",
    "config = tf.ConfigProto()\n",
    "# Use GPU in incremental mode (is ignored on CPU version)\n",
    "config.gpu_options.allow_growth=True\n",
    "# Add config=config in every tf.Session() -> tf.Session(config=config)\n",
    "\n",
    "# We have to enable eager execution to loop over the input\n",
    "#tf.enable_eager_execution() # Not working with placeholders, alternative: tf.map_fn or tf.unstack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First read in the data\n",
    "text = open(\"holy_grail.txt\", 'r').read()\n",
    "#text = text[:1000]\n",
    "text_length = len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text contains 60061 letters and 78 unique characters\n"
     ]
    }
   ],
   "source": [
    "# We create our vocabulary\n",
    "vocab = set(text)\n",
    "vocab_len = len(vocab)\n",
    "# Create the index for each character and vice versa\n",
    "char_to_pos = {char:pos for pos, char in enumerate(vocab)}\n",
    "pos_to_char = {pos:char for pos, char in enumerate(vocab)}\n",
    "\n",
    "# Print some properties of our data\n",
    "print(\"The text contains {} letters and {} unique characters\".format(text_length, vocab_len))\n",
    "\n",
    "# Transform each character to the position of that character in the vocabulary\n",
    "text_number = [char_to_pos[char] for char in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the chunk with a sub sequence length of 25\n",
    "chunks = [text_number[i:i+25] for i in range(text_length-25)]\n",
    "# Construct the input sequences and the target sequences\n",
    "input_sequences = chunks[:-1]\n",
    "target_sequences = chunks[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset for tensorflow\n",
    "\n",
    "# First we reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Now we use the tf.data library to create a tensorflow dataset\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices((input_sequences, target_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the iterator \n",
    "\n",
    "# First we create the iterator\n",
    "iterator = tf.data.Iterator.from_structure(training_dataset.output_types,\n",
    "                                           training_dataset.output_shapes)\n",
    "\n",
    "# We prepare the initializer operations for the training\n",
    "training_init_op = iterator.make_initializer(training_dataset)\n",
    "\n",
    "# We name the get_next method of the iterator to use it as a shortcut\n",
    "next_batch = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"one_hot:0\", shape=(25, 78), dtype=float32) Tensor(\"one_hot_1:0\", shape=(25, 78), dtype=float32)\n",
      "Tensor(\"Placeholder:0\", shape=(1, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# First we will format the data in the correct way\n",
    "\n",
    "# We define our input data and the corresponding target data and transform it to one hot encoding\n",
    "input_data = tf.one_hot(next_batch[0], vocab_len)\n",
    "target_data = tf.one_hot(next_batch[1], vocab_len)\n",
    "\n",
    "print(input_data, target_data)\n",
    "\n",
    "hidden_len = 100\n",
    "# Create the placeholder for the hidden state\n",
    "rem_hidden_state = tf.placeholder(tf.float32, shape=(1,hidden_len))\n",
    "\n",
    "print(rem_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we build our RNN\n",
    "\n",
    "# We have to define our lists\n",
    "hidden_states = []\n",
    "logits = []\n",
    "\n",
    "# We define all our weights and biases\n",
    "std_dev = 0.1\n",
    "w_xh = tf.Variable(tf.random_normal([vocab_len, hidden_len], stddev=std_dev))\n",
    "w_hh = tf.Variable(tf.random_normal([hidden_len, hidden_len], stddev=std_dev))\n",
    "b_h = tf.Variable(tf.zeros([1, hidden_len]))  # tf.Variable(tf.random_normal([1, hidden_len]))\n",
    "w_hy = tf.Variable(tf.random_normal([hidden_len, vocab_len], stddev=std_dev))\n",
    "b_y = tf.Variable(tf.zeros([1, vocab_len]))  # tf.Variable(tf.random_normal([1, vocab_len]))\n",
    "\n",
    "# We define the for_loop as a function \n",
    "#def forward_steps(char):\n",
    "for char in tf.unstack(input_data):\n",
    "    char = tf.reshape(char, [1, vocab_len])\n",
    "    new_hidden_state = tf.tanh(char @ w_xh + rem_hidden_state @ w_hh + b_h)\n",
    "    logit = new_hidden_state @ w_hy + b_y\n",
    "    hidden_states.append(new_hidden_state)\n",
    "    logits.append(logit)\n",
    "\n",
    "# Fancy for loop\n",
    "# tf.map_fn(forward_steps, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read out hidden states?\n",
    "rem_hidden = hidden_states[-1]\n",
    "\n",
    "# Read out logits of last iteration\n",
    "final_logits = logits[-1]\n",
    "output_softmax = tf.nn.softmax(final_logits)\n",
    "\n",
    "# Loss is mean of the cross entropies in each time step\n",
    "outputs = tf.concat(logits, axis=0)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=target_data, logits=outputs)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Optimizer\n",
    "# We create an optimizer\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "# And tell the optimizer that it should minimize the loss\n",
    "training_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----\n",
      "Ep: 0, Step: 60035, Loss: 2.3411314487457275\n",
      "----\n",
      "\n",
      " faffff igg Jus ate t!\n",
      " THURTHUR:  ARTHUAllotiglinaaN:    -boungle  by EMI AR: ot  AR:  BEDED: [ur--[nd  BEDEMI'! y\n",
      " ye  THURh!\n",
      " KEMI youglatht't'tichthangLan!\n",
      "Stoung beringAR:  at  th my  ye BEDEMI  \n",
      "\n",
      "----\n",
      "Ep: 1, Step: 120070, Loss: 2.4277563095092773\n",
      "----\n",
      "\n",
      "  Weaiman   Caintt fneHUR: GUAlet!\n",
      "   s! t!\n",
      " nked   as-mealy t.\n",
      " THUR:  is\n",
      " s t!\n",
      " do, AR: f  HURThtastpe! aue yoroung    tat!\n",
      " GUR: bimyoplotstostat\n",
      " THURTHUR:   Westhep tha spowaiges!\n",
      "     d  he R: m\n",
      "\n",
      "----\n",
      "Ep: 2, Step: 180105, Loss: 2.2901291847229004\n",
      "----\n",
      "\n",
      "shettht  BERHUR: y!\n",
      " buweciriiou t t tiged,     BEMIRTHUs, s at!\n",
      " L  neangichie!  Slyothiank  Nourikangeee,  ARTHolist CEDEMo ou     ick   ntontisyou re!\n",
      "  !\n",
      " nc AR:  ace knatannd,   [sck d  mkplond p\n",
      "\n",
      "----\n",
      "Ep: 3, Step: 240140, Loss: 2.1055526733398438\n",
      "----\n",
      "\n",
      "t!\n",
      "  THURTowsghig sat!\n",
      " N: twiginiglartder!     Nest Cou EMIR: t, EMIn!\n",
      " m.\n",
      " mat!\n",
      "    , HUR: It!\n",
      " BEMIitlite byoprcrtof t 'statthehar nd\n",
      " be\n",
      "  y sary Ithany\n",
      " yongend n. as [stor, y\n",
      " THURThetis\n",
      "\n",
      " avego\n",
      "\n",
      "----\n",
      "Ep: 4, Step: 300175, Loss: 2.0908923149108887\n",
      "----\n",
      "\n",
      "   HUR: ace s'tett ck OHURTHURDEMIRTHUR: o, yo AR: GURTHallleeasantincenkegr GURTHURThthenthe  hetlle, Wh.\n",
      " bit  t    haregisthe  tw    you oug ofte inad  s AR: belof  out  ca.\n",
      " veaf ishengg at. g Ye \n",
      "\n",
      "----\n",
      "Ep: 5, Step: 360210, Loss: 2.140545606613159\n",
      "----\n",
      "\n",
      "estyo.  GURTHUR: ARThas ou myare'geke ytithigs n THUARnd.  \n",
      "S:   ind allenceplk,\n",
      " at\n",
      " bodotelpe.\n",
      " bt!\n",
      "\n",
      " Yemyul\n",
      " d, thtin  HUR:  We!\n",
      "  GUR: BEMy, o, ou,  yoube plouourouilbotyouf  inglam or Sinces br t\n",
      "\n",
      "----\n",
      "Ep: 6, Step: 420245, Loss: 2.3253602981567383\n",
      "----\n",
      "\n",
      "!\n",
      "  ofaceusnotaseti!   afNofredy y, BEMIn tin stitis  adnd, Knd\n",
      " atiot, sotaremailacl nn, youad he bs qut ant tath tdired  obrat\n",
      " tlupeyousyes!\n",
      " ARDEMIR: BEMIR: cks!\n",
      " tetwambed AR: ARTHUR: HUBERTHUAR:\n",
      "\n",
      "----\n",
      "Ep: 7, Step: 480280, Loss: 2.109410524368286\n",
      "----\n",
      "\n",
      "y 3pppithofpth't!, ye  BEMIR:   omapaclelee-ug y Jure EMInd St. ys  ce Yerkvame, t many y EMIR: s! shou be   nany. au hr! catrest   hif  IR: BEMIR: Angy ant!\n",
      "  d s Aers ARuf,   at! catlan'ramotindatur\n",
      "\n",
      "----\n",
      "Ep: 8, Step: 540315, Loss: 2.33164644241333\n",
      "----\n",
      "\n",
      "\n",
      "  s\n",
      " y Yeyou thr ady induweabou aty y  ly fmet [thelepe we Juet iceca   mau mallll.\n",
      " Yelk  vet Hatlilkhigestimaces aad ARTHURTHURThistdplil't hal-\n",
      " at tts  HUARTHURTHUR:  spewaug f t!\n",
      " t!  'top! Gor \n",
      "\n",
      "----\n",
      "Ep: 9, Step: 600350, Loss: 2.4669950008392334\n",
      "----\n",
      "\n",
      "ow omut   ath!\n",
      "    opppesce, EDEMI  y\n",
      "  Wldoresthther hellorwimy tiu  finge oiflllliighantatt  t BEMIRTHURTTHUinot ofimeritasesthtsttyeasth! trary\n",
      " yof bure oe'tisthas   tat aringhinckes! ware!\n",
      "  vera\n"
     ]
    }
   ],
   "source": [
    "# Finally we can train our model\n",
    "\n",
    "# First we specify the number of epochs\n",
    "epochs = 10\n",
    "\n",
    "# We safely create our tensorflow session and pass our config parameters (for correct GPU usage, if GPU available)\n",
    "with tf.Session(config=config) as sess:\n",
    "    \n",
    "    # We initialize our variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # We define a step counter (for the summaries)\n",
    "    global_step = 0\n",
    "    \n",
    "    # We will run our training as often as specified in epochs\n",
    "    for ep in range(epochs):\n",
    "        \n",
    "        # Training Part\n",
    "        \n",
    "        # We initialize the hidden state with zeros in the beginning of each new epoch\n",
    "        hidden_state = np.zeros((1, hidden_len))\n",
    "        \n",
    "        # We have to load the training data into the iterator\n",
    "        sess.run(training_init_op)\n",
    "        \n",
    "        # We have to loop over all our batches in every epoch\n",
    "        while True:\n",
    "            try:\n",
    "                # print(hidden_state.shape, hidden_state.dtype)\n",
    "                # We train with one batch and read the summary and save it in the variable summary\n",
    "                _, hidden_state, loss_val = sess.run([training_step, rem_hidden, loss], feed_dict={rem_hidden_state: hidden_state})\n",
    "                # print(hidden_state)       \n",
    "                # We update our step counter\n",
    "                global_step += 1\n",
    "                \n",
    "            # After we finished all batches, we catch the OutOfRangeErrpr and break\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "                \n",
    "        # Print loss after each epoch\n",
    "        print(\"\\n----\\nEp: {}, Step: {}, Loss: {}\\n----\\n\".format(ep, global_step, loss_val))\n",
    "        \n",
    "        # Sampling\n",
    "        sample_length = 200\n",
    "        random_sub_sequences = [input_sequences[np.random.choice(len(input_sequences))]]\n",
    "        random_fake_targets = [target_sequences[np.random.choice(len(target_sequences))]]\n",
    "        #input_data = tf.one_hot(next_batch[0], vocab_len)\n",
    "        #target_data = tf.one_hot(next_batch[1], vocab_len)\n",
    "        sample_outputs = []\n",
    "        \n",
    "        for t in range(sample_length):\n",
    "            sample_dataset = tf.data.Dataset.from_tensor_slices((random_sub_sequences, random_fake_targets))\n",
    "            sess.run(iterator.make_initializer(sample_dataset))\n",
    "            sample_output_softmax, hidden_state = sess.run([output_softmax, rem_hidden], feed_dict={rem_hidden_state: hidden_state})\n",
    "            sample_output = np.random.choice(vocab_len, p=sample_output_softmax.ravel())\n",
    "            sample_outputs.append(sample_output)\n",
    "            # Delete the first character and append the newly sampled one\n",
    "            random_sub_sequences[0] = random_sub_sequences[0][1:] + [sample_output]\n",
    "        \n",
    "        # Print the sample\n",
    "        print(''.join(pos_to_char[pos] for pos in sample_outputs))           "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
