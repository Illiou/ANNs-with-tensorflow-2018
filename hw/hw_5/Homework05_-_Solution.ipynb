{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_char = open(\"holy_grail.txt\",'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5', ',', 'x', 'E', 'k', 'N', '`', '0', 'o', 'p', 'R', 'c', 'F', 'V', 'g', ']', 'z', 'J', '2', 'S', '3', 't', 's', 'i', '7', 'Q', 'D', 'L', 'I', 'u', 'H', '-', 'G', 'n', 'K', 'q', 'B', '\"', 'Z', 'P', 'y', 'j', 'A', '\\n', 'U', '!', 'M', ')', ' ', 'Y', 'v', '8', '?', 'm', '6', 'T', 'W', '4', '.', 'w', 'C', 'a', 'O', 'l', '[', ':', 'h', '(', '9', 'e', \"'\", 'f', 'r', '#', 'b', 'd', '1', ';']\n",
      "vocab size: 78\n",
      "text lenght: 60061\n"
     ]
    }
   ],
   "source": [
    "# Generate the vocabulary.\n",
    "vocab = list(set(text_char))\n",
    "vocab_size = len(vocab)\n",
    "print(vocab)\n",
    "print(\"vocab size: {}\".format(vocab_size))\n",
    "\n",
    "text_len = len(text_char)\n",
    "print(\"text lenght: {}\".format(text_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries to switch between the indices of the characters and the characters themselves.\n",
    "char_to_idx = { ch:i for i,ch in enumerate(vocab) }\n",
    "idx_to_char = { i:ch for i,ch in enumerate(vocab) }\n",
    "\n",
    "# Translate the text to indices.\n",
    "text_idx = [char_to_idx[c] for c in text_char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the lenght of sequences we want to train on.\n",
    "seq_len = 25\n",
    "\n",
    "# Generate the dataset.\n",
    "input_data = []\n",
    "target_data = []\n",
    "for i in range(text_len-seq_len):\n",
    "    input_data.append(text_idx[i:i+seq_len])\n",
    "    target_data.append(text_idx[i+1:i+seq_len+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# Create the TensorFlow dataset.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_data,target_data))\n",
    "\n",
    "# We do not train on batches or shuffle the dataset.\n",
    "iterator = tf.data.Iterator.from_structure(dataset.output_types,dataset.output_shapes)\n",
    "iterator_init_op = iterator.make_initializer(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the input from the generator.\n",
    "next_batch = iterator.get_next()\n",
    "input_data = next_batch[0]\n",
    "target_data = next_batch[1]\n",
    "\n",
    "# Create one hot tensors.\n",
    "input_one_hot = tf.one_hot(input_data, depth=vocab_size)\n",
    "target_one_hot = tf.one_hot(target_data, depth=vocab_size)\n",
    "\n",
    "# Initialize the placeholder for the hidden state.\n",
    "hidden_size = 100\n",
    "init_hs = tf.placeholder(shape=[1, hidden_size], dtype=tf.float32)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'RNN/add_2:0' shape=(1, 78) dtype=float32>, <tf.Tensor 'RNN/add_5:0' shape=(1, 78) dtype=float32>, <tf.Tensor 'RNN/add_8:0' shape=(1, 78) dtype=float32>, <tf.Tensor 'RNN/add_11:0' shape=(1, 78) dtype=float32>, <tf.Tensor 'RNN/add_14:0' shape=(1, 78) dtype=float32>, <tf.Tensor 'RNN/add_17:0' shape=(1, 78) dtype=float32>, <tf.Tensor 'RNN/add_20:0' shape=(1, 78) dtype=float32>, <tf.Tensor 'RNN/add_23:0' shape=(1, 78) dtype=float32>, <tf.Tensor 'RNN/add_26:0' shape=(1, 78) dtype=float32>, <tf.Tensor 'RNN/add_29:0' shape=(1, 78) dtype=float32>, <tf.Tensor 'RNN/add_32:0' shape=(1, 78) dtype=float32>, <tf.Tensor 'RNN/add_35:0' shape=(1, 78) dtype=float32>, <tf.Tensor 'RNN/add_38:0' shape=(1, 78) dtype=float32>, <tf.Tensor 'RNN/add_41:0' shape=(1, 78) dtype=float32>, <tf.Tensor 'RNN/add_44:0' shape=(1, 78) dtype=float32>, <tf.Tensor 'RNN/add_47:0' shape=(1, 78) dtype=float32>, <tf.Tensor 'RNN/add_50:0' shape=(1, 78) dtype=float32>, <tf.Tensor 'RNN/add_53:0' shape=(1, 78) dtype=float32>, <tf.Tensor 'RNN/add_56:0' shape=(1, 78) dtype=float32>, <tf.Tensor 'RNN/add_59:0' shape=(1, 78) dtype=float32>, <tf.Tensor 'RNN/add_62:0' shape=(1, 78) dtype=float32>, <tf.Tensor 'RNN/add_65:0' shape=(1, 78) dtype=float32>, <tf.Tensor 'RNN/add_68:0' shape=(1, 78) dtype=float32>, <tf.Tensor 'RNN/add_71:0' shape=(1, 78) dtype=float32>, <tf.Tensor 'RNN/add_74:0' shape=(1, 78) dtype=float32>]\n",
      "Tensor(\"concat:0\", shape=(25, 78), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Build the model.\n",
    "with tf.variable_scope(\"RNN\", reuse=tf.AUTO_REUSE) as scope:\n",
    "    \n",
    "    # Set hidden state.\n",
    "    hs_t = init_hs\n",
    "    # Initialize list to save the hidden states and outputs of the sequence.\n",
    "    hs = []\n",
    "    ys = []\n",
    "    \n",
    "    \n",
    "    # Initialize all weights and biases.\n",
    "    initializer = tf.random_normal_initializer(stddev=0.1)\n",
    "    Wxh = tf.get_variable(\"Wxh\", [vocab_size, hidden_size], initializer=initializer)\n",
    "    Whh = tf.get_variable(\"Whh\", [hidden_size, hidden_size], initializer=initializer)\n",
    "    Why = tf.get_variable(\"Why\", [hidden_size, vocab_size], initializer=initializer)\n",
    "    bh  = tf.get_variable(\"bh\", [hidden_size], initializer=initializer)\n",
    "    by = tf.get_variable(\"by\", [vocab_size], initializer=initializer)\n",
    "    \n",
    "    # Unfold the RNN for as many steps as our sequence is long.\n",
    "    for t in range(seq_len):\n",
    "                \n",
    "        # Read out the ith input and the ith target character\n",
    "        xs_t = input_one_hot[t,:]\n",
    "        xs_t = tf.expand_dims(xs_t, axis=0)\n",
    "        ts_t = target_one_hot[t,:]\n",
    "        \n",
    "        # Compute the new hidden state.\n",
    "        hs_t = tf.tanh(tf.matmul(xs_t, Wxh) + tf.matmul(hs_t, Whh) + bh)\n",
    "        # Compute the new output.\n",
    "        ys_t = tf.matmul(hs_t, Why) + by\n",
    "        # Store hidden state and output.\n",
    "        hs.append(hs_t)\n",
    "        ys.append(ys_t)\n",
    "        \n",
    "# The RNN is done. \n",
    "# Save the hidden state for feeding it to the next sub sequence.\n",
    "hs_remember = hs[0]\n",
    "print(ys)\n",
    "# Compute the softmax of the very last prediction for sampling.\n",
    "output_softmax = tf.nn.softmax(ys[-1])\n",
    "# Compute the loss of all the outputs.\n",
    "outputs = tf.concat(ys, axis=0)\n",
    "print(outputs)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=target_one_hot, logits=outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer.\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "training_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.837211\n",
      "----\n",
      " kEab\n",
      " bARTHUR:  Sy rhakk\n",
      "  ARTHUR:  Sa sapr\n",
      "  ARTHUR:  Stard mealacn dantacisemallote it\n",
      "  ARTHUR:  Ist!\n",
      "  BEDEMDRDM:  Yot,!  ARTHED:  Yad!  IADEETHUR:  Int!\n",
      "  ARTHUR:  Sartitl! ckdeatenkctspannous an \n",
      "----\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-61de338d953c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0;31m# Read out the hidden state for the next forward step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;31m# Do the training step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0mhs_remember_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhs_remember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minit_hs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhs_remember_val\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;31m# Increment the time step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(100):\n",
    "    \n",
    "        # Set time step counter.\n",
    "        t = 0\n",
    "        # Load the dataset into the iterator.\n",
    "        sess.run(iterator_init_op)\n",
    "        hs_remember_val = np.zeros([1,hidden_size])\n",
    "\n",
    "        # Go through the dataset until its empty.\n",
    "        while True:\n",
    "            try:\n",
    "                # If we are in the first time step intialize the hidden state with zeros.\n",
    "                #if t == 0:\n",
    "                #    hs_remember_val = np.zeros([1,hidden_size])\n",
    "                \n",
    "                # Feed in the last hidden state.\n",
    "                # Read out the loss value for printing.\n",
    "                # Read out the hidden state for the next forward step.\n",
    "                # Do the training step.\n",
    "                hs_remember_val, loss_val, _ = sess.run([hs_remember, loss, training_step], feed_dict={init_hs: hs_remember_val})\n",
    "                \n",
    "                # Increment the time step.\n",
    "                t += 1\n",
    "\n",
    "            # Stop if iterator is empty.\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "                \n",
    "          \n",
    "        # After each epoch we print the loss value.\n",
    "        print(\"Epoch: {}, Loss: {:f}\".format(epoch, loss_val))\n",
    "        \n",
    "        # The main validation procedure we will use is sampling a text from our model to see how good we \n",
    "        # approximate the original dataset.\n",
    "        \n",
    "        # How many characters would we like to sample?\n",
    "        sample_len = 200\n",
    "        \n",
    "        # Get a random starting sequence from our training dataset.\n",
    "        start_idx = random.randint(0, len(text_idx) - seq_len)\n",
    "        seq_idx = text_idx[start_idx:start_idx + seq_len]      \n",
    "      \n",
    "        # List to store the characters sampled by our model.\n",
    "        sample_seq_idx        = []\n",
    "        sample_hs_remember_val = np.zeros([1,hidden_size])\n",
    "\n",
    "        # Sample as many characters as we would like.\n",
    "        for n in range(sample_len):\n",
    "            \n",
    "            # To feed the starting sequence into our model we first need to put it into a dataset.\n",
    "            # As we do not compare anything here we need some fake target values.\n",
    "            fake_target = np.zeros([1,25], dtype=np.int32)\n",
    "            sample_dataset = tf.data.Dataset.from_tensor_slices(([seq_idx], fake_target))\n",
    "            # Load this dataset into the iterator.\n",
    "            sess.run(iterator.make_initializer(sample_dataset))\n",
    "    \n",
    "            # Now we need to read out two things. The softmax output for sampling a character and the hidden state to\n",
    "            # feed it in again.\n",
    "            sample_output_softmax_val, sample_hs_remember_val = sess.run([output_softmax, hs_remember],\n",
    "                                                                       feed_dict={init_hs: sample_hs_remember_val})\n",
    "\n",
    "            \n",
    "            \n",
    "            # Sample a character from the softmax output distribution and append it.\n",
    "            sample = np.random.choice(range(vocab_size), p=sample_output_softmax_val.ravel())\n",
    "            sample_seq_idx.append(sample)\n",
    "            # Update the start sequence for sampling the next character\n",
    "            seq_idx = seq_idx[1:] + [sample]\n",
    "        \n",
    "      \n",
    "        # Print sample.\n",
    "        sample_txt = ''.join(idx_to_char[idx] for idx in sample_seq_idx)\n",
    "        print('----\\n %s \\n----\\n' % (sample_txt,))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
