{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the text\n",
    "text = open(\"holy_grail.txt\",'r').read()\n",
    "\n",
    "# making a dictionary in which the characters correspond to numbers\n",
    "characters = set(text)\n",
    "numbers = range(0,len(characters))\n",
    "characters_numbers = zip(characters, numbers)\n",
    "dictionary = dict(characters_numbers) \n",
    "\n",
    "# making a list of numbers out of the text\n",
    "text_list = list(text)\n",
    "number_text = []\n",
    "for character in text_list:\n",
    "    number = dictionary[character]\n",
    "    number_text.append(number)\n",
    "    \n",
    "# chunking the text \n",
    "sequence_list = []\n",
    "for i in range(0,len(number_text)-25):\n",
    "    sequence_list.append(number_text[i:i+25])\n",
    "\n",
    "#lists with input and target sequences\n",
    "input_list = []\n",
    "target_list = []\n",
    "for i in range(0, len(sequence_list) - 1):\n",
    "    input_list.append(sequence_list[i])\n",
    "    target_list.append(sequence_list[i+1])\n",
    "data = np.array(input_list)\n",
    "labels = np.array(target_list)\n",
    "\n",
    "#making the dataset and iterator\n",
    "tf.reset_default_graph()\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "iterator = tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)\n",
    "\n",
    "next_batch = iterator.get_next()\n",
    "initialize_iterator = iterator.make_initializer(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the batch \n",
    "input_data = next_batch[0]\n",
    "input_data = tf.cast(input_data, tf.int32)\n",
    "labels = next_batch[1]\n",
    "labels = tf.cast(labels, tf.int32)\n",
    "\n",
    "#make a one hot vector out of labels\n",
    "one_hot_data = tf.one_hot(input_data, len(dictionary))\n",
    "one_hot_labels = tf.one_hot(labels, len(dictionary))\n",
    "\n",
    "#making a hidden state for remembered hidden state\n",
    "hidden_state_placeholder = tf.placeholder(tf.float32, shape=(1,100))\n",
    "\n",
    "#empty lists to save hidden list and logits in \n",
    "hidden_states = []\n",
    "logits = []\n",
    "\n",
    "#defining weights and biases\n",
    "wxh = tf.Variable(tf.random_normal([len(dictionary), 100], stddev=0.1))\n",
    "whh = tf.Variable(tf.random_normal([100, 100], stddev=0.1))\n",
    "why = tf.Variable(tf.random_normal([100, len(dictionary)], stddev=0.1))\n",
    "bh = tf.Variable(tf.random_normal([1, 100], stddev=0.1))\n",
    "by = tf.Variable(tf.random_normal([1, len(dictionary)], stddev=0.1))\n",
    "\n",
    "#iterate through the subsequence \n",
    "for i in range(0,25):\n",
    "    #input_character = one_hot_data[i]\n",
    "    input_character = tf.reshape(one_hot_data[i], [1, len(dictionary)])\n",
    "#for input_character in one_hot_data:\n",
    "    next_hidden_state = tf.tanh(input_character @ wxh + hidden_state_placeholder @ whh + bh)\n",
    "    hidden_states.append(next_hidden_state)\n",
    "    logit = next_hidden_state @ why + by\n",
    "    logits.append(logit)\n",
    "    \n",
    "#getting results\n",
    "last_hidden_state = hidden_states[-1]\n",
    "last_logits = logits[-1]\n",
    "output = tf.nn.softmax(last_logits)\n",
    "\n",
    "#calculating loss\n",
    "outputs = tf.concat(logits, axis=0)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=one_hot_labels, logits=outputs)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "#define the optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "\n",
    "#define the training step\n",
    "training_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the epochs\n",
    "epochs = 10\n",
    "\n",
    "#starting Tensorflow session\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    #inizializing variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    #making global step counter\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(epochs):               \n",
    "        # inizializing hidden_state_placeholder\n",
    "        hidden_state = np.zeros((1,100))           \n",
    "        #initializing iterator with training data for this epoch\n",
    "        sess.run(initialize_iterator)\n",
    "        #going through batches once\n",
    "        while True:\n",
    "            try:\n",
    "                #running training step\n",
    "                _, hidden_state, loss_val  = sess.run([training_step, last_hidden_state, loss], feed_dict={hidden_state_placeholder: hidden_state})\n",
    "                #incrementing global step\n",
    "                global_step = global_step + 1\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "                \n",
    "        #SAMPLING\n",
    "    \n",
    "        #giving information about sample\n",
    "        print('Epoch: ', epoch, ', Step: ', global_step, ', Loss: ', loss_val)\n",
    "        \n",
    "        #random subsequence\n",
    "        random_number = np.random.choice(len(input_list))\n",
    "        random_subsequence = input_list[random_number]\n",
    "        #targets are only needed for iterator\n",
    "        random_targets = target_list[random_number]\n",
    "        \n",
    "        #feeding subsequence into RNN\n",
    "        subsequence_dataset = tf.data.Dataset.from_tensor_slices(random_subsequence, random_targets)\n",
    "        \n",
    "        #list for storing all sampled characters\n",
    "        all_sampled_characters = []\n",
    "        #sampling 200 characters\n",
    "        for i in range(0,200):\n",
    "            #feeding subsequence into RNN\n",
    "            subsequence_dataset = tf.data.Dataset.from_tensor_slices(random_subsequence, random_targets)\n",
    "            sess.run(iterator.make_initializer(subsequence_dataset))\n",
    "            subsequence_softmax_output, hidden_state = sess.run([output_softmax, last_hidden_state], feed_dict={last_hidden_state: hidden_state})\n",
    "            #choosing character from output of last softmax \n",
    "            sample_character = np.random.choice(a=len(dictionary), p=subsequence_softmax_output)\n",
    "            #save sampled character\n",
    "            all_sampled_characters.append(sample_character)\n",
    "            #needed for sampling next character: first character of subsequence deleted, new sampled one appended\n",
    "            random_subsequence.append(sample_character)\n",
    "            random_subsequence.pop(0)\n",
    "            \n",
    "        #printing sampled characters\n",
    "        for character in all_sampled_characters:\n",
    "            print(dictionary[character], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
